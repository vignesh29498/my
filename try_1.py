# -*- coding: utf-8 -*-
"""try_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fND0JTaaVKmfiEpXrvn5S0sk9ej-mBDj
"""

import pandas as pd
import glob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

def load_and_merge_data(raw_folder, meaning_folder):
    raw_files = glob.glob(f'{raw_folder}/raw*.xlsx')
    meaning_files = glob.glob(f'{meaning_folder}/meaning*.xlsx')

    raw_files.sort()
    meaning_files.sort()

    all_data = []

    for raw_file, meaning_file in zip(raw_files, meaning_files):
        raw_data = pd.read_excel(raw_file)
        meaning_data = pd.read_excel(meaning_file)
        combined_data = pd.merge(raw_data, meaning_data, on='Text ID')
        all_data.append(combined_data)

    return pd.concat(all_data, ignore_index=True)

def preprocess_data(data):
    X = data['Raw Text']
    y = data['Meaning']
    vectorizer = TfidfVectorizer()
    X_tfidf = vectorizer.fit_transform(X)
    return X_tfidf, y, vectorizer

def train_model(X_tfidf, y):
    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)
    model = LogisticRegression()
    model.fit(X_train, y_train)
    return model

def predict_meanings(model, vectorizer, new_raw_file, output_file):
    new_raw_data = pd.read_excel(new_raw_file)
    new_X_tfidf = vectorizer.transform(new_raw_data['Raw Text'])
    predicted_meanings = model.predict(new_X_tfidf)

    predictions_df = pd.DataFrame({
        'Text ID': new_raw_data['Text ID'],
        'Raw Text': new_raw_data['Raw Text'],
        'Predicted Meaning': predicted_meanings
    })

    predictions_df.to_excel(output_file, index=False)

if __name__ == "__main__":
    raw_folder = '/content/drive/MyDrive/CITI/Raw'
    meaning_folder = 'path/to/meaning/files'
    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'
    output_file = '/content/drive/MyDrive/CITI/predicted_meanings.xlsx'

    # Load and merge data
    data = load_and_merge_data(raw_folder, meaning_folder)

    # Preprocess data
    X_tfidf, y, vectorizer = preprocess_data(data)

    # Train model
    model = train_model(X_tfidf, y)

    # Predict and save results
    predict_meanings(model, vectorizer, new_raw_file, output_file)

import pandas as pd
import glob
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

def load_and_merge_data(raw_folder, meaning_folder):
    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))
    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))

    if not raw_files or not meaning_files:
        print("No files found. Please check your file paths and patterns.")
        return pd.DataFrame()  # Return an empty DataFrame

    raw_files.sort()
    meaning_files.sort()

    all_data = []

    for raw_file, meaning_file in zip(raw_files, meaning_files):
        raw_data = pd.read_excel(raw_file)
        meaning_data = pd.read_excel(meaning_file)
        combined_data = pd.merge(raw_data, meaning_data, on='Text ID')
        all_data.append(combined_data)

    if all_data:
        return pd.concat(all_data, ignore_index=True)
    else:
        return pd.DataFrame()  # Return an empty DataFrame if no data was combined

def preprocess_data(data):
    if data.empty:
        print("No data to preprocess. Exiting.")
        return None, None, None

    X = data['Raw Text']
    y = data['Meaning']
    vectorizer = TfidfVectorizer()
    X_tfidf = vectorizer.fit_transform(X)
    return X_tfidf, y, vectorizer

def train_model(X_tfidf, y):
    if X_tfidf is None or y is None:
        print("No data to train. Exiting.")
        return None

    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)
    model = LogisticRegression()
    model.fit(X_train, y_train)
    return model

def predict_meanings(model, vectorizer, new_raw_file, output_file):
    if model is None or vectorizer is None:
        print("Model or vectorizer not available. Exiting.")
        return

    new_raw_data = pd.read_excel(new_raw_file)
    new_X_tfidf = vectorizer.transform(new_raw_data['Raw Text'])
    predicted_meanings = model.predict(new_X_tfidf)

    predictions_df = pd.DataFrame({
        'Text ID': new_raw_data['Text ID'],
        'Raw Text': new_raw_data['Raw Text'],
        'Predicted Meaning': predicted_meanings
    })

    predictions_df.to_excel(output_file, index=False)

if __name__ == "__main__":
    raw_folder = '/content/drive/MyDrive/CITI/Raw'
    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'
    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'
    output_file = '/content/drive/MyDrive/CITI/predicted_meanings.xlsx'

    # Load and merge data
    data = load_and_merge_data(raw_folder, meaning_folder)

    if data.empty:
        print("No data to process. Please check your input files.")
    else:
        # Preprocess data
        X_tfidf, y, vectorizer = preprocess_data(data)

        # Train model
        model = train_model(X_tfidf, y)

        # Predict and save results
        predict_meanings(model, vectorizer, new_raw_file, output_file)

import pandas as pd
import glob
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

def load_and_merge_data(raw_folder, meaning_folder):
    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))
    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))

    if not raw_files or not meaning_files:
        print("No files found. Please check your file paths and patterns.")
        return pd.DataFrame()  # Return an empty DataFrame

    raw_files.sort()
    meaning_files.sort()

    all_data = []

    for raw_file, meaning_file in zip(raw_files, meaning_files):
        raw_data = pd.read_excel(raw_file)
        meaning_data = pd.read_excel(meaning_file)

        # Debug: print columns of each file
        print(f"Raw file '{raw_file}' columns: {raw_data.columns}")
        print(f"Meaning file '{meaning_file}' columns: {meaning_data.columns}")

        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:
            print(f"Missing 'Text ID' in {raw_file} or {meaning_file}")
            continue

        combined_data = pd.merge(raw_data, meaning_data, on='Text ID')
        all_data.append(combined_data)

    if all_data:
        return pd.concat(all_data, ignore_index=True)
    else:
        return pd.DataFrame()  # Return an empty DataFrame if no data was combined

def preprocess_data(data):
    if data.empty:
        print("No data to preprocess. Exiting.")
        return None, None, None

    # Convert 'Raw Text' column to string
    data['Raw Text'] = data['Raw Text'].astype(str)

    X = data['Raw Text']
    y = data['Meaning']
    vectorizer = TfidfVectorizer()
    X_tfidf = vectorizer.fit_transform(X)
    return X_tfidf, y, vectorizer

def train_model(X_tfidf, y):
    if X_tfidf is None or y is None:
        print("No data to train. Exiting.")
        return None

    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)
    model = LogisticRegression()
    model.fit(X_train, y_train)
    return model

def predict_meanings(model, vectorizer, new_raw_file, output_file):
    if model is None or vectorizer is None:
        print("Model or vectorizer not available. Exiting.")
        return

    new_raw_data = pd.read_excel(new_raw_file)
    if 'Text ID' not in new_raw_data.columns or 'Raw Text' not in new_raw_data.columns:
        print(f"New raw file is missing required columns: {new_raw_file}")
        return

    # Convert 'Raw Text' column to string
    new_raw_data['Raw Text'] = new_raw_data['Raw Text'].astype(str)

    new_X_tfidf = vectorizer.transform(new_raw_data['Raw Text'])
    predicted_meanings = model.predict(new_X_tfidf)

    predictions_df = pd.DataFrame({
        'Text ID': new_raw_data['Text ID'],
        'Raw Text': new_raw_data['Raw Text'],
        'Predicted Meaning': predicted_meanings
    })

    predictions_df.to_excel(output_file, index=False)

if __name__ == "__main__":
    raw_folder = '/content/drive/MyDrive/CITI/Raw'
    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'
    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'
    output_file = '/content/drive/MyDrive/CITI/predicted_meanings.xlsx'

    # Load and merge data
    data = load_and_merge_data(raw_folder, meaning_folder)

    if data.empty:
        print("No data to process. Please check your input files.")
    else:
        # Preprocess data
        X_tfidf, y, vectorizer = preprocess_data(data)

        # Train model
        model = train_model(X_tfidf, y)

        # Predict and save results
        predict_meanings(model, vectorizer, new_raw_file, output_file)

"""Working"""

import pandas as pd
import glob
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

def load_and_merge_data(raw_folder, meaning_folder):
    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))
    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))

    if not raw_files or not meaning_files:
        print("No files found. Please check your file paths and patterns.")
        return pd.DataFrame()  # Return an empty DataFrame

    raw_files.sort()
    meaning_files.sort()

    all_data = []

    for raw_file, meaning_file in zip(raw_files, meaning_files):
        raw_data = pd.read_excel(raw_file)
        meaning_data = pd.read_excel(meaning_file)

        # Debug: print columns of each file
        print(f"Raw file '{raw_file}' columns: {raw_data.columns}")
        print(f"Meaning file '{meaning_file}' columns: {meaning_data.columns}")

        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:
            print(f"Missing 'Text ID' in {raw_file} or {meaning_file}")
            continue

        combined_data = pd.merge(raw_data, meaning_data, on='Text ID')
        all_data.append(combined_data)

    if all_data:
        return pd.concat(all_data, ignore_index=True)
    else:
        return pd.DataFrame()  # Return an empty DataFrame if no data was combined

def preprocess_data(data):
    if data.empty:
        print("No data to preprocess. Exiting.")
        return None, None, None

    # Convert 'Raw Text' column to string
    data['Raw Text'] = data['Raw Text'].astype(str)

    X = data['Raw Text']
    y = data['Meaning']
    vectorizer = TfidfVectorizer()
    X_tfidf = vectorizer.fit_transform(X)
    return X_tfidf, y, vectorizer

def train_model(X_tfidf, y):
    if X_tfidf is None or y is None:
        print("No data to train. Exiting.")
        return None

    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)
    model = LogisticRegression()
    model.fit(X_train, y_train)
    return model

def predict_meanings(model, vectorizer, new_raw_file, output_file):
    if model is None or vectorizer is None:
        print("Model or vectorizer not available. Exiting.")
        return

    new_raw_data = pd.read_excel(new_raw_file)
    if 'Text ID' not in new_raw_data.columns or 'Raw Text' not in new_raw_data.columns:
        print(f"New raw file is missing required columns: {new_raw_file}")
        return

    # Convert 'Raw Text' column to string
    new_raw_data['Raw Text'] = new_raw_data['Raw Text'].astype(str)

    new_X_tfidf = vectorizer.transform(new_raw_data['Raw Text'])
    predicted_meanings = model.predict(new_X_tfidf)

    predictions_df = pd.DataFrame({
        'Text ID': new_raw_data['Text ID'],
        'Raw Text': new_raw_data['Raw Text'],
        'Predicted Meaning': predicted_meanings
    })

    predictions_df.to_excel(output_file, index=False)

if __name__ == "__main__":
    raw_folder = '/content/drive/MyDrive/CITI/Raw'
    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'
    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'
    output_file = '/content/drive/MyDrive/CITI/predicted_meanings.xlsx'

    # Load and merge data
    data = load_and_merge_data(raw_folder, meaning_folder)

    if data.empty:
        print("No data to process. Please check your input files.")
    else:
        # Preprocess data
        X_tfidf, y, vectorizer = preprocess_data(data)

        # Train model
        model = train_model(X_tfidf, y)

        # Predict and save results
        predict_meanings(model, vectorizer, new_raw_file, output_file)

import pandas as pd
import glob
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

def load_and_merge_data(raw_folder, meaning_folder):
    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))
    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))

    if not raw_files or not meaning_files:
        print("No files found. Please check your file paths and patterns.")
        return pd.DataFrame()  # Return an empty DataFrame

    raw_files.sort()
    meaning_files.sort()

    all_data = []

    for raw_file, meaning_file in zip(raw_files, meaning_files):
        raw_data = pd.read_excel(raw_file)
        meaning_data = pd.read_excel(meaning_file)

        # Debug: print columns of each file
        print(f"Raw file '{raw_file}' columns: {raw_data.columns}")
        print(f"Meaning file '{meaning_file}' columns: {meaning_data.columns}")

        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:
            print(f"Missing 'Text ID' in {raw_file} or {meaning_file}")
            continue

        # Combine the Raw Text, Raw Text 1, and Raw Text 2 columns into a single 'Raw Text' column
        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].fillna('').agg(' '.join, axis=1)

        # Combine the Meaning, Meaning 1, and Meaning 2 columns into a single 'Meaning' column
        meaning_data['Meaning'] = meaning_data[['Meaning', 'Meaning 1', 'Meaning 2']].fillna('').agg(' '.join, axis=1)

        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')
        all_data.append(combined_data)

    if all_data:
        return pd.concat(all_data, ignore_index=True)
    else:
        return pd.DataFrame()  # Return an empty DataFrame if no data was combined

def preprocess_data(data):
    if data.empty:
        print("No data to preprocess. Exiting.")
        return None, None, None

    # Convert 'Raw Text' column to string
    data['Raw Text'] = data['Raw Text'].astype(str)

    X = data['Raw Text']
    y = data['Meaning']

    return X, y

def train_model(X, y):
    if X is None or y is None:
        print("No data to train. Exiting.")
        return None

    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression
    model_pipeline = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('clf', LogisticRegression(max_iter=1000))
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model_pipeline.fit(X_train, y_train)

    # Debug: Print model accuracy on the test set
    accuracy = model_pipeline.score(X_test, y_test)
    print(f"Model accuracy on test set: {accuracy:.2f}")

    return model_pipeline

def predict_meanings(model, new_raw_file, output_file):
    if model is None:
        print("Model not available. Exiting.")
        return

    new_raw_data = pd.read_excel(new_raw_file)
    if 'Text ID' not in new_raw_data.columns:
        print(f"New raw file is missing 'Text ID' column: {new_raw_file}")
        return

    # Combine the Raw Text, Raw Text 1, and Raw Text 2 columns into a single 'Raw Text' column
    new_raw_data['Raw Text'] = new_raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].fillna('').agg(' '.join, axis=1)

    predicted_meanings = model.predict(new_raw_data['Raw Text'])

    predictions_df = pd.DataFrame({
        'Text ID': new_raw_data['Text ID'],
        'Raw Text': new_raw_data['Raw Text'],
        'Predicted Meaning': predicted_meanings
    })

    predictions_df.to_excel(output_file, index=False)
    print(f"Predictions saved to {output_file}")

if __name__ == "__main__":
    raw_folder = '/content/drive/MyDrive/CITI/Raw'
    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'
    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'
    output_file = '/content/drive/MyDrive/CITI/predicted_meanings1.xlsx'

    # Load and merge data
    data = load_and_merge_data(raw_folder, meaning_folder)

    if data.empty:
        print("No data to process. Please check your input files.")
    else:
        # Preprocess data
        X, y = preprocess_data(data)

        # Train model
        model = train_model(X, y)

        # Predict and save results
        predict_meanings(model, new_raw_file, output_file)

import pandas as pd
import glob
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

def load_and_merge_data(raw_folder, meaning_folder):
    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))
    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))

    if not raw_files or not meaning_files:
        print("No files found. Please check your file paths and patterns.")
        return pd.DataFrame()  # Return an empty DataFrame

    raw_files.sort()
    meaning_files.sort()

    all_data = []

    for raw_file, meaning_file in zip(raw_files, meaning_files):
        raw_data = pd.read_excel(raw_file)
        meaning_data = pd.read_excel(meaning_file)

        # Debug: print columns of each file
        print(f"Raw file '{raw_file}' columns: {raw_data.columns}")
        print(f"Meaning file '{meaning_file}' columns: {meaning_data.columns}")

        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:
            print(f"Missing 'Text ID' in {raw_file} or {meaning_file}")
            continue

        # Ensure all columns are strings before concatenation
        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)
        meaning_data['Meaning'] = meaning_data[['Meaning', 'Meaning 1', 'Meaning 2']].astype(str).agg(' '.join, axis=1)

        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')
        all_data.append(combined_data)

    if all_data:
        return pd.concat(all_data, ignore_index=True)
    else:
        return pd.DataFrame()  # Return an empty DataFrame if no data was combined

def preprocess_data(data):
    if data.empty:
        print("No data to preprocess. Exiting.")
        return None, None, None

    # Convert 'Raw Text' column to string
    data['Raw Text'] = data['Raw Text'].astype(str)

    X = data['Raw Text']
    y = data['Meaning']

    return X, y

def train_model(X, y):
    if X is None or y is None:
        print("No data to train. Exiting.")
        return None

    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression
    model_pipeline = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('clf', LogisticRegression(max_iter=1000))
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model_pipeline.fit(X_train, y_train)

    # Debug: Print model accuracy on the test set
    accuracy = model_pipeline.score(X_test, y_test)
    print(f"Model accuracy on test set: {accuracy:.2f}")

    return model_pipeline

def predict_meanings(model, new_raw_file, output_file):
    if model is None:
        print("Model not available. Exiting.")
        return

    new_raw_data = pd.read_excel(new_raw_file)
    if 'Text ID' not in new_raw_data.columns:
        print(f"New raw file is missing 'Text ID' column: {new_raw_file}")
        return

    # Ensure all columns are strings before concatenation
    new_raw_data['Raw Text'] = new_raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)

    predicted_meanings = model.predict(new_raw_data['Raw Text'])

    predictions_df = pd.DataFrame({
        'Text ID': new_raw_data['Text ID'],
        'Raw Text': new_raw_data['Raw Text'],
        'Predicted Meaning': predicted_meanings
    })

    predictions_df.to_excel(output_file, index=False)
    print(f"Predictions saved to {output_file}")

if __name__ == "__main__":
    raw_folder = '/content/drive/MyDrive/CITI/Raw'
    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'
    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'
    output_file = '/content/drive/MyDrive/CITI/predicted_meanings1.xlsx'

    # Load and merge data
    data = load_and_merge_data(raw_folder, meaning_folder)

    if data.empty:
        print("No data to process. Please check your input files.")
    else:
        # Preprocess data
        X, y = preprocess_data(data)

        # Train model
        model = train_model(X, y)

        # Predict and save results
        predict_meanings(model, new_raw_file, output_file)

import pandas as pd
import glob
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

def load_and_merge_data(raw_folder, meaning_folder):
    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))
    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))

    if not raw_files or not meaning_files:
        print("No files found. Please check your file paths and patterns.")
        return pd.DataFrame()  # Return an empty DataFrame

    raw_files.sort()
    meaning_files.sort()

    all_data = []

    for raw_file, meaning_file in zip(raw_files, meaning_files):
        raw_data = pd.read_excel(raw_file)
        meaning_data = pd.read_excel(meaning_file)

        # Debug: print columns of each file
        print(f"Raw file '{raw_file}' columns: {raw_data.columns}")
        print(f"Meaning file '{meaning_file}' columns: {meaning_data.columns}")

        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:
            print(f"Missing 'Text ID' in {raw_file} or {meaning_file}")
            continue

        # Ensure all columns are strings before concatenation
        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)

        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')
        all_data.append(combined_data)

    if all_data:
        return pd.concat(all_data, ignore_index=True)
    else:
        return pd.DataFrame()  # Return an empty DataFrame if no data was combined

def preprocess_data(data):
    if data.empty:
        print("No data to preprocess. Exiting.")
        return None, None, None

    # Convert 'Raw Text' column to string
    data['Raw Text'] = data['Raw Text'].astype(str)

    X = data['Raw Text']
    y = data['Meaning']

    return X, y

def train_model(X, y):
    if X is None or y is None:
        print("No data to train. Exiting.")
        return None

    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression
    model_pipeline = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('clf', LogisticRegression(max_iter=1000))
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model_pipeline.fit(X_train, y_train)

    # Debug: Print model accuracy on the test set
    accuracy = model_pipeline.score(X_test, y_test)
    print(f"Model accuracy on test set: {accuracy:.2f}")

    return model_pipeline

def predict_meanings(model, new_raw_file, output_file):
    if model is None:
        print("Model not available. Exiting.")
        return

    new_raw_data = pd.read_excel(new_raw_file)
    if 'Text ID' not in new_raw_data.columns:
        print(f"New raw file is missing 'Text ID' column: {new_raw_file}")
        return

    # Ensure all columns are strings before concatenation
    new_raw_data['Raw Text'] = new_raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)

    predicted_meanings = model.predict(new_raw_data['Raw Text'])
    predicted_probabilities = model.predict_proba(new_raw_data['Raw Text'])  # If needed

    # Map predictions to meaningful names based on conditions
    new_raw_data['Predicted Meaning'] = predicted_meanings
    new_raw_data['Prediction Category'] = new_raw_data['Predicted Meaning'].apply(lambda x: {
        'ID its Partial id numaric': 'Partial ID',
        'Country AB is a country text': 'Country Text',
        'Full_ID This is full ID numaric': 'Full ID'
    }.get(x, 'Other'))  # Default to 'Other' if prediction doesn't match

    new_raw_data.to_excel(output_file, index=False)
    print(f"Predictions saved to {output_file}")

if __name__ == "__main__":
    raw_folder = '/content/drive/MyDrive/CITI/Raw'
    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'
    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'
    output_file = '/content/drive/MyDrive/CITI/predicted_meanings1.xlsx'

    # Load and merge data
    data = load_and_merge_data(raw_folder, meaning_folder)

    if data.empty:
        print("No data to process. Please check your input files.")
    else:
        # Preprocess data
        X, y = preprocess_data(data)

        # Train model
        model = train_model(X, y)

        # Predict and save results
        predict_meanings(model, new_raw_file, output_file)

import pandas as pd
import glob
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.externals import joblib  # For model saving (deprecated in scikit-learn 0.24)

def load_and_merge_data(raw_folder, meaning_folder):
    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))
    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))

    if not raw_files or not meaning_files:
        print("No files found. Please check your file paths and patterns.")
        return pd.DataFrame()  # Return an empty DataFrame

    raw_files.sort()
    meaning_files.sort()

    all_data = []

    for raw_file, meaning_file in zip(raw_files, meaning_files):
        raw_data = pd.read_excel(raw_file)
        meaning_data = pd.read_excel(meaning_file)

        # Debug: print columns of each file
        print(f"Raw file '{raw_file}' columns: {raw_data.columns}")
        print(f"Meaning file '{meaning_file}' columns: {meaning_data.columns}")

        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:
            print(f"Missing 'Text ID' in {raw_file} or {meaning_file}")
            continue

        # Ensure all columns are strings before concatenation
        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)

        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')
        all_data.append(combined_data)

    if all_data:
        return pd.concat(all_data, ignore_index=True)
    else:
        return pd.DataFrame()  # Return an empty DataFrame if no data was combined

def preprocess_data(data):
    if data.empty:
        print("No data to preprocess. Exiting.")
        return None, None, None

    # Convert 'Raw Text' column to string
    data['Raw Text'] = data['Raw Text'].astype(str)

    X = data['Raw Text']
    y = data['Meaning']

    return X, y

def train_model(X, y):
    if X is None or y is None:
        print("No data to train. Exiting.")
        return None

    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression
    model_pipeline = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('clf', LogisticRegression(max_iter=1000))
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model_pipeline.fit(X_train, y_train)

    # Debug: Print model accuracy on the test set
    accuracy = model_pipeline.score(X_test, y_test)
    print(f"Model accuracy on test set: {accuracy:.2f}")

    # Save the trained model to a file
    model_filename = 'text_pattern_model.pkl'  # Name of the model file
    model_path = os.path.join('path/to/save/model', model_filename)  # Replace with your desired path
    joblib.dump(model_pipeline, model_path)
    print(f"Trained model saved to {model_path}")

    return model_pipeline

if __name__ == "__main__":
    raw_folder = 'path/to/raw/files'
    meaning_folder = 'path/to/meaning/files'

    # Load and merge data
    data = load_and_merge_data(raw_folder, meaning_folder)

    if data.empty:
        print("No data to process. Please check your input files.")
    else:
        # Preprocess data
        X, y = preprocess_data(data)

        # Train model
        model = train_model(X, y)

"""FInal

"""

!pip install pandas scikit-learn openpyxl joblib

!pip install joblib
!pip install scikit-learn

import pandas as pd
import glob
import os
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
import joblib  # Correct import for joblib

def load_and_merge_data(raw_folder, meaning_folder):
    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))
    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))

    if not raw_files or not meaning_files:
        print("No files found. Please check your file paths and patterns.")
        return pd.DataFrame()  # Return an empty DataFrame

    raw_files.sort()
    meaning_files.sort()

    all_data = []

    for raw_file, meaning_file in zip(raw_files, meaning_files):
        raw_data = pd.read_excel(raw_file)
        meaning_data = pd.read_excel(meaning_file)

        # Debug: print columns of each file
        print(f"Raw file '{raw_file}' columns: {raw_data.columns}")
        print(f"Meaning file '{meaning_file}' columns: {meaning_data.columns}")

        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:
            print(f"Missing 'Text ID' in {raw_file} or {meaning_file}")
            continue

        # Ensure all columns are strings before concatenation
        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)

        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')
        all_data.append(combined_data)

    if all_data:
        return pd.concat(all_data, ignore_index=True)
    else:
        return pd.DataFrame()  # Return an empty DataFrame if no data was combined

def preprocess_data(data):
    if data.empty:
        print("No data to preprocess. Exiting.")
        return None, None, None

    # Convert 'Raw Text' column to string
    data['Raw Text'] = data['Raw Text'].astype(str)

    X = data['Raw Text']
    y = data['Meaning']

    return X, y

def train_model(X, y):
    if X is None or y is None:
        print("No data to train. Exiting.")
        return None

    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression
    model_pipeline = Pipeline([
        ('tfidf', TfidfVectorizer()),
        ('clf', LogisticRegression(max_iter=1000))
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model_pipeline.fit(X_train, y_train)

    # Debug: Print model accuracy on the test set
    accuracy = model_pipeline.score(X_test, y_test)
    print(f"Model accuracy on test set: {accuracy:.2f}")

    # Save the trained model to a file using joblib
    model_filename = 'text_pattern_model.pkl'  # Name of the model file
    model_path = os.path.join('/content/drive/MyDrive/CITI', model_filename)  # Replace with your desired path
    joblib.dump(model_pipeline, model_path)
    print(f"Trained model saved to {model_path}")

    return model_pipeline

def predict_meanings(model, new_raw_file, output_file):
    if model is None:
        print("Model not available. Exiting.")
        return

    new_raw_data = pd.read_excel(new_raw_file)
    if 'Text ID' not in new_raw_data.columns:
        print(f"New raw file is missing 'Text ID' column: {new_raw_file}")
        return

    # Ensure all columns are strings before concatenation
    new_raw_data['Raw Text'] = new_raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)

    # Predict meanings using the trained model
    predicted_meanings = model.predict(new_raw_data['Raw Text'])

    # Add predicted meanings to the new_raw_data DataFrame
    new_raw_data['Predicted Meaning'] = predicted_meanings

    # Save predictions to an output file
    new_raw_data.to_excel(output_file, index=False)
    print(f"Predictions saved to {output_file}")

if __name__ == "__main__":
    raw_folder = '/content/drive/MyDrive/CITI/Raw'
    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'
    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'
    output_file = '/content/drive/MyDrive/CITI/predicted.xlsx'

    # Load and merge data
    data = load_and_merge_data(raw_folder, meaning_folder)

    if data.empty:
        print("No data to process. Please check your input files.")
    else:
        # Preprocess data
        X, y = preprocess_data(data)

        # Train model
        model = train_model(X, y)

        # Predict and save results from the new raw file
        predict_meanings(model, new_raw_file, output_file)