{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "-eK2qnTBcr6i",
        "outputId": "9611b9ad-d3a3-4521-95a4-ad83876ea599"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-85c64327af95>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Load and merge data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_merge_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeaning_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-85c64327af95>\u001b[0m in \u001b[0;36mload_and_merge_data\u001b[0;34m(raw_folder, meaning_folder)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def load_and_merge_data(raw_folder, meaning_folder):\n",
        "    raw_files = glob.glob(f'{raw_folder}/raw*.xlsx')\n",
        "    meaning_files = glob.glob(f'{meaning_folder}/meaning*.xlsx')\n",
        "\n",
        "    raw_files.sort()\n",
        "    meaning_files.sort()\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for raw_file, meaning_file in zip(raw_files, meaning_files):\n",
        "        raw_data = pd.read_excel(raw_file)\n",
        "        meaning_data = pd.read_excel(meaning_file)\n",
        "        combined_data = pd.merge(raw_data, meaning_data, on='Text ID')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    return pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "def preprocess_data(data):\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = vectorizer.fit_transform(X)\n",
        "    return X_tfidf, y, vectorizer\n",
        "\n",
        "def train_model(X_tfidf, y):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def predict_meanings(model, vectorizer, new_raw_file, output_file):\n",
        "    new_raw_data = pd.read_excel(new_raw_file)\n",
        "    new_X_tfidf = vectorizer.transform(new_raw_data['Raw Text'])\n",
        "    predicted_meanings = model.predict(new_X_tfidf)\n",
        "\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Text ID': new_raw_data['Text ID'],\n",
        "        'Raw Text': new_raw_data['Raw Text'],\n",
        "        'Predicted Meaning': predicted_meanings\n",
        "    })\n",
        "\n",
        "    predictions_df.to_excel(output_file, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_folder = '/content/drive/MyDrive/CITI/Raw'\n",
        "    meaning_folder = 'path/to/meaning/files'\n",
        "    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'\n",
        "    output_file = '/content/drive/MyDrive/CITI/predicted_meanings.xlsx'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(raw_folder, meaning_folder)\n",
        "\n",
        "    # Preprocess data\n",
        "    X_tfidf, y, vectorizer = preprocess_data(data)\n",
        "\n",
        "    # Train model\n",
        "    model = train_model(X_tfidf, y)\n",
        "\n",
        "    # Predict and save results\n",
        "    predict_meanings(model, vectorizer, new_raw_file, output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def load_and_merge_data(raw_folder, meaning_folder):\n",
        "    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))\n",
        "    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))\n",
        "\n",
        "    if not raw_files or not meaning_files:\n",
        "        print(\"No files found. Please check your file paths and patterns.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "    raw_files.sort()\n",
        "    meaning_files.sort()\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for raw_file, meaning_file in zip(raw_files, meaning_files):\n",
        "        raw_data = pd.read_excel(raw_file)\n",
        "        meaning_data = pd.read_excel(meaning_file)\n",
        "        combined_data = pd.merge(raw_data, meaning_data, on='Text ID')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data was combined\n",
        "\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        print(\"No data to preprocess. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = vectorizer.fit_transform(X)\n",
        "    return X_tfidf, y, vectorizer\n",
        "\n",
        "def train_model(X_tfidf, y):\n",
        "    if X_tfidf is None or y is None:\n",
        "        print(\"No data to train. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def predict_meanings(model, vectorizer, new_raw_file, output_file):\n",
        "    if model is None or vectorizer is None:\n",
        "        print(\"Model or vectorizer not available. Exiting.\")\n",
        "        return\n",
        "\n",
        "    new_raw_data = pd.read_excel(new_raw_file)\n",
        "    new_X_tfidf = vectorizer.transform(new_raw_data['Raw Text'])\n",
        "    predicted_meanings = model.predict(new_X_tfidf)\n",
        "\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Text ID': new_raw_data['Text ID'],\n",
        "        'Raw Text': new_raw_data['Raw Text'],\n",
        "        'Predicted Meaning': predicted_meanings\n",
        "    })\n",
        "\n",
        "    predictions_df.to_excel(output_file, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_folder = '/content/drive/MyDrive/CITI/Raw'\n",
        "    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'\n",
        "    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'\n",
        "    output_file = '/content/drive/MyDrive/CITI/predicted_meanings.xlsx'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(raw_folder, meaning_folder)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"No data to process. Please check your input files.\")\n",
        "    else:\n",
        "        # Preprocess data\n",
        "        X_tfidf, y, vectorizer = preprocess_data(data)\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(X_tfidf, y)\n",
        "\n",
        "        # Predict and save results\n",
        "        predict_meanings(model, vectorizer, new_raw_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "esp2O8omf9JP",
        "outputId": "51f13b7b-2e36-475b-e90f-fcad34847453"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'int' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f41edf47e5cf>\u001b[0m in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mX_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f41edf47e5cf>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Meaning'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mX_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1386\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def load_and_merge_data(raw_folder, meaning_folder):\n",
        "    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))\n",
        "    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))\n",
        "\n",
        "    if not raw_files or not meaning_files:\n",
        "        print(\"No files found. Please check your file paths and patterns.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "    raw_files.sort()\n",
        "    meaning_files.sort()\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for raw_file, meaning_file in zip(raw_files, meaning_files):\n",
        "        raw_data = pd.read_excel(raw_file)\n",
        "        meaning_data = pd.read_excel(meaning_file)\n",
        "\n",
        "        # Debug: print columns of each file\n",
        "        print(f\"Raw file '{raw_file}' columns: {raw_data.columns}\")\n",
        "        print(f\"Meaning file '{meaning_file}' columns: {meaning_data.columns}\")\n",
        "\n",
        "        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:\n",
        "            print(f\"Missing 'Text ID' in {raw_file} or {meaning_file}\")\n",
        "            continue\n",
        "\n",
        "        combined_data = pd.merge(raw_data, meaning_data, on='Text ID')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data was combined\n",
        "\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        print(\"No data to preprocess. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    data['Raw Text'] = data['Raw Text'].astype(str)\n",
        "\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = vectorizer.fit_transform(X)\n",
        "    return X_tfidf, y, vectorizer\n",
        "\n",
        "def train_model(X_tfidf, y):\n",
        "    if X_tfidf is None or y is None:\n",
        "        print(\"No data to train. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def predict_meanings(model, vectorizer, new_raw_file, output_file):\n",
        "    if model is None or vectorizer is None:\n",
        "        print(\"Model or vectorizer not available. Exiting.\")\n",
        "        return\n",
        "\n",
        "    new_raw_data = pd.read_excel(new_raw_file)\n",
        "    if 'Text ID' not in new_raw_data.columns or 'Raw Text' not in new_raw_data.columns:\n",
        "        print(f\"New raw file is missing required columns: {new_raw_file}\")\n",
        "        return\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    new_raw_data['Raw Text'] = new_raw_data['Raw Text'].astype(str)\n",
        "\n",
        "    new_X_tfidf = vectorizer.transform(new_raw_data['Raw Text'])\n",
        "    predicted_meanings = model.predict(new_X_tfidf)\n",
        "\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Text ID': new_raw_data['Text ID'],\n",
        "        'Raw Text': new_raw_data['Raw Text'],\n",
        "        'Predicted Meaning': predicted_meanings\n",
        "    })\n",
        "\n",
        "    predictions_df.to_excel(output_file, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_folder = '/content/drive/MyDrive/CITI/Raw'\n",
        "    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'\n",
        "    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'\n",
        "    output_file = '/content/drive/MyDrive/CITI/predicted_meanings.xlsx'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(raw_folder, meaning_folder)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"No data to process. Please check your input files.\")\n",
        "    else:\n",
        "        # Preprocess data\n",
        "        X_tfidf, y, vectorizer = preprocess_data(data)\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(X_tfidf, y)\n",
        "\n",
        "        # Predict and save results\n",
        "        predict_meanings(model, vectorizer, new_raw_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B_2DJHrjiMm",
        "outputId": "48f4b2dd-2f73-4c31-f510-5be3e7020ca6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw1.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning1.xlsx' columns: Index(['Text ID', 'Meaning', 'Unnamed: 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw2.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning2.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw3.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning3.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw4.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning4.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Working"
      ],
      "metadata": {
        "id": "qGQ6D0-qoyHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def load_and_merge_data(raw_folder, meaning_folder):\n",
        "    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))\n",
        "    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))\n",
        "\n",
        "    if not raw_files or not meaning_files:\n",
        "        print(\"No files found. Please check your file paths and patterns.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "    raw_files.sort()\n",
        "    meaning_files.sort()\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for raw_file, meaning_file in zip(raw_files, meaning_files):\n",
        "        raw_data = pd.read_excel(raw_file)\n",
        "        meaning_data = pd.read_excel(meaning_file)\n",
        "\n",
        "        # Debug: print columns of each file\n",
        "        print(f\"Raw file '{raw_file}' columns: {raw_data.columns}\")\n",
        "        print(f\"Meaning file '{meaning_file}' columns: {meaning_data.columns}\")\n",
        "\n",
        "        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:\n",
        "            print(f\"Missing 'Text ID' in {raw_file} or {meaning_file}\")\n",
        "            continue\n",
        "\n",
        "        combined_data = pd.merge(raw_data, meaning_data, on='Text ID')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data was combined\n",
        "\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        print(\"No data to preprocess. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    data['Raw Text'] = data['Raw Text'].astype(str)\n",
        "\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X_tfidf = vectorizer.fit_transform(X)\n",
        "    return X_tfidf, y, vectorizer\n",
        "\n",
        "def train_model(X_tfidf, y):\n",
        "    if X_tfidf is None or y is None:\n",
        "        print(\"No data to train. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "def predict_meanings(model, vectorizer, new_raw_file, output_file):\n",
        "    if model is None or vectorizer is None:\n",
        "        print(\"Model or vectorizer not available. Exiting.\")\n",
        "        return\n",
        "\n",
        "    new_raw_data = pd.read_excel(new_raw_file)\n",
        "    if 'Text ID' not in new_raw_data.columns or 'Raw Text' not in new_raw_data.columns:\n",
        "        print(f\"New raw file is missing required columns: {new_raw_file}\")\n",
        "        return\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    new_raw_data['Raw Text'] = new_raw_data['Raw Text'].astype(str)\n",
        "\n",
        "    new_X_tfidf = vectorizer.transform(new_raw_data['Raw Text'])\n",
        "    predicted_meanings = model.predict(new_X_tfidf)\n",
        "\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Text ID': new_raw_data['Text ID'],\n",
        "        'Raw Text': new_raw_data['Raw Text'],\n",
        "        'Predicted Meaning': predicted_meanings\n",
        "    })\n",
        "\n",
        "    predictions_df.to_excel(output_file, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_folder = '/content/drive/MyDrive/CITI/Raw'\n",
        "    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'\n",
        "    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'\n",
        "    output_file = '/content/drive/MyDrive/CITI/predicted_meanings.xlsx'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(raw_folder, meaning_folder)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"No data to process. Please check your input files.\")\n",
        "    else:\n",
        "        # Preprocess data\n",
        "        X_tfidf, y, vectorizer = preprocess_data(data)\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(X_tfidf, y)\n",
        "\n",
        "        # Predict and save results\n",
        "        predict_meanings(model, vectorizer, new_raw_file, output_file)\n"
      ],
      "metadata": {
        "id": "mRB2L8_Jmijf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def load_and_merge_data(raw_folder, meaning_folder):\n",
        "    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))\n",
        "    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))\n",
        "\n",
        "    if not raw_files or not meaning_files:\n",
        "        print(\"No files found. Please check your file paths and patterns.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "    raw_files.sort()\n",
        "    meaning_files.sort()\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for raw_file, meaning_file in zip(raw_files, meaning_files):\n",
        "        raw_data = pd.read_excel(raw_file)\n",
        "        meaning_data = pd.read_excel(meaning_file)\n",
        "\n",
        "        # Debug: print columns of each file\n",
        "        print(f\"Raw file '{raw_file}' columns: {raw_data.columns}\")\n",
        "        print(f\"Meaning file '{meaning_file}' columns: {meaning_data.columns}\")\n",
        "\n",
        "        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:\n",
        "            print(f\"Missing 'Text ID' in {raw_file} or {meaning_file}\")\n",
        "            continue\n",
        "\n",
        "        # Combine the Raw Text, Raw Text 1, and Raw Text 2 columns into a single 'Raw Text' column\n",
        "        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].fillna('').agg(' '.join, axis=1)\n",
        "\n",
        "        # Combine the Meaning, Meaning 1, and Meaning 2 columns into a single 'Meaning' column\n",
        "        meaning_data['Meaning'] = meaning_data[['Meaning', 'Meaning 1', 'Meaning 2']].fillna('').agg(' '.join, axis=1)\n",
        "\n",
        "        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data was combined\n",
        "\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        print(\"No data to preprocess. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    data['Raw Text'] = data['Raw Text'].astype(str)\n",
        "\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def train_model(X, y):\n",
        "    if X is None or y is None:\n",
        "        print(\"No data to train. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression\n",
        "    model_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Debug: Print model accuracy on the test set\n",
        "    accuracy = model_pipeline.score(X_test, y_test)\n",
        "    print(f\"Model accuracy on test set: {accuracy:.2f}\")\n",
        "\n",
        "    return model_pipeline\n",
        "\n",
        "def predict_meanings(model, new_raw_file, output_file):\n",
        "    if model is None:\n",
        "        print(\"Model not available. Exiting.\")\n",
        "        return\n",
        "\n",
        "    new_raw_data = pd.read_excel(new_raw_file)\n",
        "    if 'Text ID' not in new_raw_data.columns:\n",
        "        print(f\"New raw file is missing 'Text ID' column: {new_raw_file}\")\n",
        "        return\n",
        "\n",
        "    # Combine the Raw Text, Raw Text 1, and Raw Text 2 columns into a single 'Raw Text' column\n",
        "    new_raw_data['Raw Text'] = new_raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].fillna('').agg(' '.join, axis=1)\n",
        "\n",
        "    predicted_meanings = model.predict(new_raw_data['Raw Text'])\n",
        "\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Text ID': new_raw_data['Text ID'],\n",
        "        'Raw Text': new_raw_data['Raw Text'],\n",
        "        'Predicted Meaning': predicted_meanings\n",
        "    })\n",
        "\n",
        "    predictions_df.to_excel(output_file, index=False)\n",
        "    print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_folder = '/content/drive/MyDrive/CITI/Raw'\n",
        "    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'\n",
        "    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'\n",
        "    output_file = '/content/drive/MyDrive/CITI/predicted_meanings1.xlsx'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(raw_folder, meaning_folder)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"No data to process. Please check your input files.\")\n",
        "    else:\n",
        "        # Preprocess data\n",
        "        X, y = preprocess_data(data)\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(X, y)\n",
        "\n",
        "        # Predict and save results\n",
        "        predict_meanings(model, new_raw_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "06Oc6UUdoOF3",
        "outputId": "da808c89-5f46-4776-8fe4-efac52980e60"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw1.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning1.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "sequence item 0: expected str instance, int found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-86291767a8bb>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Load and merge data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_merge_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeaning_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-86291767a8bb>\u001b[0m in \u001b[0;36mload_and_merge_data\u001b[0;34m(raw_folder, meaning_folder)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Combine the Raw Text, Raw Text 1, and Raw Text 2 columns into a single 'Raw Text' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Raw Text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Raw Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Raw Text 1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Raw Text 2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Combine the Meaning, Meaning 1, and Meaning 2 columns into a single 'Meaning' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, axis, *args, **kwargs)\u001b[0m\n\u001b[1;32m   9194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9195\u001b[0m         \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9196\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrelabeling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   9421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9422\u001b[0m         )\n\u001b[0;32m-> 9423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"apply\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   9424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9425\u001b[0m     def applymap(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def load_and_merge_data(raw_folder, meaning_folder):\n",
        "    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))\n",
        "    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))\n",
        "\n",
        "    if not raw_files or not meaning_files:\n",
        "        print(\"No files found. Please check your file paths and patterns.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "    raw_files.sort()\n",
        "    meaning_files.sort()\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for raw_file, meaning_file in zip(raw_files, meaning_files):\n",
        "        raw_data = pd.read_excel(raw_file)\n",
        "        meaning_data = pd.read_excel(meaning_file)\n",
        "\n",
        "        # Debug: print columns of each file\n",
        "        print(f\"Raw file '{raw_file}' columns: {raw_data.columns}\")\n",
        "        print(f\"Meaning file '{meaning_file}' columns: {meaning_data.columns}\")\n",
        "\n",
        "        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:\n",
        "            print(f\"Missing 'Text ID' in {raw_file} or {meaning_file}\")\n",
        "            continue\n",
        "\n",
        "        # Ensure all columns are strings before concatenation\n",
        "        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)\n",
        "        meaning_data['Meaning'] = meaning_data[['Meaning', 'Meaning 1', 'Meaning 2']].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data was combined\n",
        "\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        print(\"No data to preprocess. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    data['Raw Text'] = data['Raw Text'].astype(str)\n",
        "\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def train_model(X, y):\n",
        "    if X is None or y is None:\n",
        "        print(\"No data to train. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression\n",
        "    model_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Debug: Print model accuracy on the test set\n",
        "    accuracy = model_pipeline.score(X_test, y_test)\n",
        "    print(f\"Model accuracy on test set: {accuracy:.2f}\")\n",
        "\n",
        "    return model_pipeline\n",
        "\n",
        "def predict_meanings(model, new_raw_file, output_file):\n",
        "    if model is None:\n",
        "        print(\"Model not available. Exiting.\")\n",
        "        return\n",
        "\n",
        "    new_raw_data = pd.read_excel(new_raw_file)\n",
        "    if 'Text ID' not in new_raw_data.columns:\n",
        "        print(f\"New raw file is missing 'Text ID' column: {new_raw_file}\")\n",
        "        return\n",
        "\n",
        "    # Ensure all columns are strings before concatenation\n",
        "    new_raw_data['Raw Text'] = new_raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "    predicted_meanings = model.predict(new_raw_data['Raw Text'])\n",
        "\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'Text ID': new_raw_data['Text ID'],\n",
        "        'Raw Text': new_raw_data['Raw Text'],\n",
        "        'Predicted Meaning': predicted_meanings\n",
        "    })\n",
        "\n",
        "    predictions_df.to_excel(output_file, index=False)\n",
        "    print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_folder = '/content/drive/MyDrive/CITI/Raw'\n",
        "    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'\n",
        "    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'\n",
        "    output_file = '/content/drive/MyDrive/CITI/predicted_meanings1.xlsx'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(raw_folder, meaning_folder)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"No data to process. Please check your input files.\")\n",
        "    else:\n",
        "        # Preprocess data\n",
        "        X, y = preprocess_data(data)\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(X, y)\n",
        "\n",
        "        # Predict and save results\n",
        "        predict_meanings(model, new_raw_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flPPl_c2o0KO",
        "outputId": "2f50bd7e-d718-4230-8787-223bb306a581"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw1.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning1.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw2.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning2.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw3.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning3.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw4.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning4.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Model accuracy on test set: 1.00\n",
            "Predictions saved to /content/drive/MyDrive/CITI/predicted_meanings1.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "def load_and_merge_data(raw_folder, meaning_folder):\n",
        "    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))\n",
        "    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))\n",
        "\n",
        "    if not raw_files or not meaning_files:\n",
        "        print(\"No files found. Please check your file paths and patterns.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "    raw_files.sort()\n",
        "    meaning_files.sort()\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for raw_file, meaning_file in zip(raw_files, meaning_files):\n",
        "        raw_data = pd.read_excel(raw_file)\n",
        "        meaning_data = pd.read_excel(meaning_file)\n",
        "\n",
        "        # Debug: print columns of each file\n",
        "        print(f\"Raw file '{raw_file}' columns: {raw_data.columns}\")\n",
        "        print(f\"Meaning file '{meaning_file}' columns: {meaning_data.columns}\")\n",
        "\n",
        "        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:\n",
        "            print(f\"Missing 'Text ID' in {raw_file} or {meaning_file}\")\n",
        "            continue\n",
        "\n",
        "        # Ensure all columns are strings before concatenation\n",
        "        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data was combined\n",
        "\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        print(\"No data to preprocess. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    data['Raw Text'] = data['Raw Text'].astype(str)\n",
        "\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def train_model(X, y):\n",
        "    if X is None or y is None:\n",
        "        print(\"No data to train. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression\n",
        "    model_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Debug: Print model accuracy on the test set\n",
        "    accuracy = model_pipeline.score(X_test, y_test)\n",
        "    print(f\"Model accuracy on test set: {accuracy:.2f}\")\n",
        "\n",
        "    return model_pipeline\n",
        "\n",
        "def predict_meanings(model, new_raw_file, output_file):\n",
        "    if model is None:\n",
        "        print(\"Model not available. Exiting.\")\n",
        "        return\n",
        "\n",
        "    new_raw_data = pd.read_excel(new_raw_file)\n",
        "    if 'Text ID' not in new_raw_data.columns:\n",
        "        print(f\"New raw file is missing 'Text ID' column: {new_raw_file}\")\n",
        "        return\n",
        "\n",
        "    # Ensure all columns are strings before concatenation\n",
        "    new_raw_data['Raw Text'] = new_raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "    predicted_meanings = model.predict(new_raw_data['Raw Text'])\n",
        "    predicted_probabilities = model.predict_proba(new_raw_data['Raw Text'])  # If needed\n",
        "\n",
        "    # Map predictions to meaningful names based on conditions\n",
        "    new_raw_data['Predicted Meaning'] = predicted_meanings\n",
        "    new_raw_data['Prediction Category'] = new_raw_data['Predicted Meaning'].apply(lambda x: {\n",
        "        'ID its Partial id numaric': 'Partial ID',\n",
        "        'Country AB is a country text': 'Country Text',\n",
        "        'Full_ID This is full ID numaric': 'Full ID'\n",
        "    }.get(x, 'Other'))  # Default to 'Other' if prediction doesn't match\n",
        "\n",
        "    new_raw_data.to_excel(output_file, index=False)\n",
        "    print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_folder = '/content/drive/MyDrive/CITI/Raw'\n",
        "    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'\n",
        "    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'\n",
        "    output_file = '/content/drive/MyDrive/CITI/predicted_meanings1.xlsx'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(raw_folder, meaning_folder)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"No data to process. Please check your input files.\")\n",
        "    else:\n",
        "        # Preprocess data\n",
        "        X, y = preprocess_data(data)\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(X, y)\n",
        "\n",
        "        # Predict and save results\n",
        "        predict_meanings(model, new_raw_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RygPesAJpbeW",
        "outputId": "673e0fe1-caca-4b7d-e1a8-6060ebb6cb6d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw1.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning1.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw2.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning2.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw3.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning3.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw4.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning4.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Model accuracy on test set: 1.00\n",
            "Predictions saved to /content/drive/MyDrive/CITI/predicted_meanings1.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.externals import joblib  # For model saving (deprecated in scikit-learn 0.24)\n",
        "\n",
        "def load_and_merge_data(raw_folder, meaning_folder):\n",
        "    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))\n",
        "    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))\n",
        "\n",
        "    if not raw_files or not meaning_files:\n",
        "        print(\"No files found. Please check your file paths and patterns.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "    raw_files.sort()\n",
        "    meaning_files.sort()\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for raw_file, meaning_file in zip(raw_files, meaning_files):\n",
        "        raw_data = pd.read_excel(raw_file)\n",
        "        meaning_data = pd.read_excel(meaning_file)\n",
        "\n",
        "        # Debug: print columns of each file\n",
        "        print(f\"Raw file '{raw_file}' columns: {raw_data.columns}\")\n",
        "        print(f\"Meaning file '{meaning_file}' columns: {meaning_data.columns}\")\n",
        "\n",
        "        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:\n",
        "            print(f\"Missing 'Text ID' in {raw_file} or {meaning_file}\")\n",
        "            continue\n",
        "\n",
        "        # Ensure all columns are strings before concatenation\n",
        "        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data was combined\n",
        "\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        print(\"No data to preprocess. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    data['Raw Text'] = data['Raw Text'].astype(str)\n",
        "\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def train_model(X, y):\n",
        "    if X is None or y is None:\n",
        "        print(\"No data to train. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression\n",
        "    model_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Debug: Print model accuracy on the test set\n",
        "    accuracy = model_pipeline.score(X_test, y_test)\n",
        "    print(f\"Model accuracy on test set: {accuracy:.2f}\")\n",
        "\n",
        "    # Save the trained model to a file\n",
        "    model_filename = 'text_pattern_model.pkl'  # Name of the model file\n",
        "    model_path = os.path.join('path/to/save/model', model_filename)  # Replace with your desired path\n",
        "    joblib.dump(model_pipeline, model_path)\n",
        "    print(f\"Trained model saved to {model_path}\")\n",
        "\n",
        "    return model_pipeline\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_folder = 'path/to/raw/files'\n",
        "    meaning_folder = 'path/to/meaning/files'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(raw_folder, meaning_folder)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"No data to process. Please check your input files.\")\n",
        "    else:\n",
        "        # Preprocess data\n",
        "        X, y = preprocess_data(data)\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(X, y)\n"
      ],
      "metadata": {
        "id": "PHdBU3h8q4ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FInal\n"
      ],
      "metadata": {
        "id": "id-pV74ErZrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn openpyxl joblib\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFcsLQ7VrgG3",
        "outputId": "4e5c8228-7184-4c0f-ba02-117d594f8cf2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install joblib\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3ycrpN3rxbZ",
        "outputId": "c7beedc0-1224-4bf4-a1b1-534db61bfa9d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib  # Correct import for joblib\n",
        "\n",
        "def load_and_merge_data(raw_folder, meaning_folder):\n",
        "    raw_files = glob.glob(os.path.join(raw_folder, 'raw*.xlsx'))\n",
        "    meaning_files = glob.glob(os.path.join(meaning_folder, 'meaning*.xlsx'))\n",
        "\n",
        "    if not raw_files or not meaning_files:\n",
        "        print(\"No files found. Please check your file paths and patterns.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame\n",
        "\n",
        "    raw_files.sort()\n",
        "    meaning_files.sort()\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for raw_file, meaning_file in zip(raw_files, meaning_files):\n",
        "        raw_data = pd.read_excel(raw_file)\n",
        "        meaning_data = pd.read_excel(meaning_file)\n",
        "\n",
        "        # Debug: print columns of each file\n",
        "        print(f\"Raw file '{raw_file}' columns: {raw_data.columns}\")\n",
        "        print(f\"Meaning file '{meaning_file}' columns: {meaning_data.columns}\")\n",
        "\n",
        "        if 'Text ID' not in raw_data.columns or 'Text ID' not in meaning_data.columns:\n",
        "            print(f\"Missing 'Text ID' in {raw_file} or {meaning_file}\")\n",
        "            continue\n",
        "\n",
        "        # Ensure all columns are strings before concatenation\n",
        "        raw_data['Raw Text'] = raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "        combined_data = pd.merge(raw_data[['Text ID', 'Raw Text']], meaning_data[['Text ID', 'Meaning']], on='Text ID')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data was combined\n",
        "\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        print(\"No data to preprocess. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    data['Raw Text'] = data['Raw Text'].astype(str)\n",
        "\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def train_model(X, y):\n",
        "    if X is None or y is None:\n",
        "        print(\"No data to train. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression\n",
        "    model_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Debug: Print model accuracy on the test set\n",
        "    accuracy = model_pipeline.score(X_test, y_test)\n",
        "    print(f\"Model accuracy on test set: {accuracy:.2f}\")\n",
        "\n",
        "    # Save the trained model to a file using joblib\n",
        "    model_filename = 'text_pattern_model.pkl'  # Name of the model file\n",
        "    model_path = os.path.join('/content/drive/MyDrive/CITI', model_filename)  # Replace with your desired path\n",
        "    joblib.dump(model_pipeline, model_path)\n",
        "    print(f\"Trained model saved to {model_path}\")\n",
        "\n",
        "    return model_pipeline\n",
        "\n",
        "def predict_meanings(model, new_raw_file, output_file):\n",
        "    if model is None:\n",
        "        print(\"Model not available. Exiting.\")\n",
        "        return\n",
        "\n",
        "    new_raw_data = pd.read_excel(new_raw_file)\n",
        "    if 'Text ID' not in new_raw_data.columns:\n",
        "        print(f\"New raw file is missing 'Text ID' column: {new_raw_file}\")\n",
        "        return\n",
        "\n",
        "    # Ensure all columns are strings before concatenation\n",
        "    new_raw_data['Raw Text'] = new_raw_data[['Raw Text', 'Raw Text 1', 'Raw Text 2']].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "    # Predict meanings using the trained model\n",
        "    predicted_meanings = model.predict(new_raw_data['Raw Text'])\n",
        "\n",
        "    # Add predicted meanings to the new_raw_data DataFrame\n",
        "    new_raw_data['Predicted Meaning'] = predicted_meanings\n",
        "\n",
        "    # Save predictions to an output file\n",
        "    new_raw_data.to_excel(output_file, index=False)\n",
        "    print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    raw_folder = '/content/drive/MyDrive/CITI/Raw'\n",
        "    meaning_folder = '/content/drive/MyDrive/CITI/Meaning'\n",
        "    new_raw_file = '/content/drive/MyDrive/CITI/raw1.xlsx'\n",
        "    output_file = '/content/drive/MyDrive/CITI/predicted.xlsx'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(raw_folder, meaning_folder)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"No data to process. Please check your input files.\")\n",
        "    else:\n",
        "        # Preprocess data\n",
        "        X, y = preprocess_data(data)\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(X, y)\n",
        "\n",
        "        # Predict and save results from the new raw file\n",
        "        predict_meanings(model, new_raw_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSe3OAWcrbNL",
        "outputId": "a028df68-65d3-4306-add3-e7cf5af629b9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw1.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning1.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw2.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning2.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw3.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning3.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Raw file '/content/drive/MyDrive/CITI/Raw/raw4.xlsx' columns: Index(['Text ID', 'Raw Text', 'Raw Text 1', 'Raw Text 2'], dtype='object')\n",
            "Meaning file '/content/drive/MyDrive/CITI/Meaning/meaning4.xlsx' columns: Index(['Text ID', 'Meaning', 'Meaning 1', 'Meaning 2'], dtype='object')\n",
            "Model accuracy on test set: 1.00\n",
            "Trained model saved to /content/drive/MyDrive/CITI/text_pattern_model.pkl\n",
            "Predictions saved to /content/drive/MyDrive/CITI/predicted.xlsx\n"
          ]
        }
      ]
    }
  ]
}