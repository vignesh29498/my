{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“ Document Chunking, Embedding, and Retrieval\n",
        "This notebook demonstrates different chunking methods (fixed, sentence, schematic, code) using **spaCy** and **regex**.\n",
        "We build TF-IDF vectors and store them locally (no external DBs).\n",
        "Then we retrieve relevant chunks based on input queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (51163182.py, line 2)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip3 install scikit-learn spacy\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# ðŸ“¦ Install required dependencies\n",
        "pip3 install scikit-learn spacy\n",
        "python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# âœ… Load spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    raise OSError(\"spaCy model 'en_core_web_sm' not found. Run: python3 -m spacy download en_core_web_sm\")\n",
        "\n",
        "# âœ… Fixed-size word chunks\n",
        "def fixed_chunk(text, chunk_size=200):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# âœ… Sentence-based chunks (spaCy)\n",
        "def sentence_chunk(text, max_sentences=3):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    return [\n",
        "        \" \".join(sentences[i:i+max_sentences])\n",
        "        for i in range(0, len(sentences), max_sentences)\n",
        "    ]\n",
        "\n",
        "# âœ… Schematic chunks (split by headings/numbers)\n",
        "def schematic_chunk(text):\n",
        "    parts = re.split(r'(?m)^#{1,6}\\s.*|^\\d+\\.\\s', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# âœ… Code chunks (split ``` blocks)\n",
        "def code_chunk(text):\n",
        "    parts = re.split(r'```.*?```', text, flags=re.S)\n",
        "    return [p.strip() for p in parts if p.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”¹ Fixed chunk: ['1. User enters username and password. System validates input. Then', 'the system sends an OTP. The user enters the OTP.', 'System verifies and logs in. ```python print(\"Hello World\") ```']\n",
            "ðŸ”¹ Sentence chunk: ['1. User enters username and password.', 'System validates input. Then the system sends an OTP.', 'The user enters the OTP. System verifies and logs in.', '```python\\nprint(\"Hello World\")\\n```']\n",
            "ðŸ”¹ Schematic chunk: ['User enters username and password.\\nSystem validates input.\\nThen the system sends an OTP.\\nThe user enters the OTP.\\nSystem verifies and logs in.\\n\\n```python\\nprint(\"Hello World\")\\n```']\n",
            "ðŸ”¹ Code chunk: ['1. User enters username and password.\\nSystem validates input.\\nThen the system sends an OTP.\\nThe user enters the OTP.\\nSystem verifies and logs in.']\n"
          ]
        }
      ],
      "source": [
        "# ðŸ“„ Example input text\n",
        "text = \"\"\"\n",
        "1. User enters username and password.\n",
        "System validates input.\n",
        "Then the system sends an OTP.\n",
        "The user enters the OTP.\n",
        "System verifies and logs in.\n",
        "\n",
        "```python\n",
        "print(\"Hello World\")\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "print(\"ðŸ”¹ Fixed chunk:\", fixed_chunk(text, chunk_size=10))\n",
        "print(\"ðŸ”¹ Sentence chunk:\", sentence_chunk(text, max_sentences=2))\n",
        "print(\"ðŸ”¹ Schematic chunk:\", schematic_chunk(text))\n",
        "print(\"ðŸ”¹ Code chunk:\", code_chunk(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# âš¡ Build TF-IDF index\n",
        "def build_tfidf_index(chunks):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def retrieve(query, chunks, vectorizer, tfidf_matrix, top_k=3):\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    top_indices = similarities.argsort()[::-1][:top_k]\n",
        "    return [(chunks[i], similarities[i]) for i in top_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: How does the system login the user?\n",
            "\n",
            "ðŸ”¹ Score: 0.6588\n",
            "The user enters the OTP. System verifies and logs in.\n",
            "\n",
            "ðŸ”¹ Score: 0.4358\n",
            "System validates input. Then the system sends an OTP.\n",
            "\n",
            "ðŸ”¹ Score: 0.1637\n",
            "1. User enters username and password.\n"
          ]
        }
      ],
      "source": [
        "# âœ… Choose chunking method\n",
        "chunks = sentence_chunk(text, max_sentences=2)\n",
        "\n",
        "# âœ… Build TF-IDF index\n",
        "vectorizer, tfidf_matrix = build_tfidf_index(chunks)\n",
        "\n",
        "# ðŸ” Query\n",
        "query = \"How does the system login the user?\"\n",
        "results = retrieve(query, chunks, vectorizer, tfidf_matrix)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "for chunk, score in results:\n",
        "    print(f\"\\nðŸ”¹ Score: {score:.4f}\\n{chunk}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "47440e23",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# âœ… Embedding model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load sentence transformer (local)\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# âœ… Load spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    raise OSError(\"spaCy model 'en_core_web_sm' not found. Run: python3 -m spacy download en_core_web_sm\")\n",
        "\n",
        "# ---------- Chunking Methods ----------\n",
        "def fixed_chunk(text, chunk_size=200):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "def sentence_chunk(text, max_sentences=3):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    return [\" \".join(sentences[i:i+max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
        "\n",
        "def schematic_chunk(text):\n",
        "    parts = re.split(r'(?m)^#{1,6}\\s.*|^\\d+\\.\\s', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "def code_chunk(text):\n",
        "    parts = re.split(r'```.*?```', text, flags=re.S)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# ---------- Build Vector Index ----------\n",
        "def build_vector_index(chunks):\n",
        "    # Embedding for each chunk\n",
        "    embeddings = embedder.encode(chunks)\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# ---------- TF-IDF Index ----------\n",
        "def build_tfidf_index(chunks):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "# ---------- Retrieval ----------\n",
        "def retrieve(query, chunks, embeddings, tfidf_vectorizer, tfidf_matrix, top_k=3):\n",
        "    # Query embedding\n",
        "    query_vec = embedder.encode([query])\n",
        "\n",
        "    # Cosine similarity with embedding vectors\n",
        "    sim_scores = cosine_similarity(query_vec, embeddings)[0]\n",
        "    top_idx_embed = np.argsort(sim_scores)[::-1][:top_k]\n",
        "\n",
        "    # TF-IDF similarity\n",
        "    tfidf_query = tfidf_vectorizer.transform([query])\n",
        "    tfidf_scores = cosine_similarity(tfidf_query, tfidf_matrix)[0]\n",
        "    top_idx_tfidf = np.argsort(tfidf_scores)[::-1][:top_k]\n",
        "\n",
        "    print(\"\\nðŸ”¹ Embedding-based results:\")\n",
        "    for idx in top_idx_embed:\n",
        "        print(f\"- {chunks[idx][:80]}... (score: {sim_scores[idx]:.4f})\")\n",
        "\n",
        "    print(\"\\nðŸ”¹ TF-IDF results:\")\n",
        "    for idx in top_idx_tfidf:\n",
        "        print(f\"- {chunks[idx][:80]}... (score: {tfidf_scores[idx]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fc456d21",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ”¹ Embedding-based results:\n",
            "- System validates OTP and logs in.... (score: 0.6073)\n",
            "- 1. User enters username and password.... (score: 0.5722)\n",
            "- 2. System validates input.... (score: 0.4879)\n",
            "\n",
            "ðŸ”¹ TF-IDF results:\n",
            "- User enters OTP. 5.... (score: 0.4692)\n",
            "- 1. User enters username and password.... (score: 0.3122)\n",
            "- 3. System sends OTP.\n",
            "4.... (score: 0.3106)\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "1. User enters username and password.\n",
        "2. System validates input.\n",
        "3. System sends OTP.\n",
        "4. User enters OTP.\n",
        "5. System validates OTP and logs in.\n",
        "\"\"\"\n",
        "\n",
        "# Choose chunking method\n",
        "chunks = sentence_chunk(text, max_sentences=2)\n",
        "\n",
        "# Build indexes\n",
        "embeddings = build_vector_index(chunks)\n",
        "tfidf_vectorizer, tfidf_matrix = build_tfidf_index(chunks)\n",
        "\n",
        "# Query\n",
        "retrieve(\"How does the system validate user login?\", chunks, embeddings, tfidf_vectorizer, tfidf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4683725",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c70963",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d47ba91",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba62a70",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b5a4adc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load spaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    raise OSError(\"Run: python3 -m spacy download en_core_web_sm\")\n",
        "\n",
        "# Embedding model\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ---------- Different Chunking Methods ----------\n",
        "\n",
        "# 1. Fixed-size word chunks\n",
        "def fixed_chunk(text, chunk_size=200):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# 2. Sentence-based chunks (spaCy)\n",
        "def sentence_chunk(text, max_sentences=3):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    return [\" \".join(sentences[i:i+max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
        "\n",
        "# 3. Schematic chunks (headings / numbered lists)\n",
        "def schematic_chunk(text):\n",
        "    parts = re.split(r'(?m)^#{1,6}\\s.*|^\\d+\\.\\s', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# 4. Code-aware chunks (split ``` blocks)\n",
        "def code_chunk(text):\n",
        "    parts = re.split(r'```.*?```', text, flags=re.S)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# 5. Semantic chunks (embedding-based, greedy merge)\n",
        "def semantic_chunk(text, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Uses embeddings to merge semantically similar sentences into chunks.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    embeddings = embedder.encode(sentences)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = [sentences[0]]\n",
        "    prev_vec = embeddings[0]\n",
        "\n",
        "    for i in range(1, len(sentences)):\n",
        "        sim = cosine_similarity([prev_vec], [embeddings[i]])[0][0]\n",
        "        if sim >= threshold:  # semantically close â†’ same chunk\n",
        "            current_chunk.append(sentences[i])\n",
        "        else:  # new semantic group\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [sentences[i]]\n",
        "        prev_vec = embeddings[i]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# ---------- Chunking Agent ----------\n",
        "def chunking_agent(text, method=\"sentence\", **kwargs):\n",
        "    if method == \"fixed\":\n",
        "        return fixed_chunk(text, **kwargs)\n",
        "    elif method == \"sentence\":\n",
        "        return sentence_chunk(text, **kwargs)\n",
        "    elif method == \"schematic\":\n",
        "        return schematic_chunk(text)\n",
        "    elif method == \"code\":\n",
        "        return code_chunk(text)\n",
        "    elif method == \"semantic\":\n",
        "        return semantic_chunk(text, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown chunking method: {method}\")\n",
        "\n",
        "# ---------- Indexing ----------\n",
        "def build_indexes(chunks):\n",
        "    embeddings = embedder.encode(chunks)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "    return embeddings, vectorizer, tfidf_matrix\n",
        "\n",
        "# ---------- Retrieval ----------\n",
        "def retrieve(query, chunks, embeddings, vectorizer, tfidf_matrix, top_k=3):\n",
        "    query_vec = embedder.encode([query])\n",
        "    sim_scores = cosine_similarity(query_vec, embeddings)[0]\n",
        "    top_idx_embed = np.argsort(sim_scores)[::-1][:top_k]\n",
        "\n",
        "    tfidf_query = vectorizer.transform([query])\n",
        "    tfidf_scores = cosine_similarity(tfidf_query, tfidf_matrix)[0]\n",
        "    top_idx_tfidf = np.argsort(tfidf_scores)[::-1][:top_k]\n",
        "\n",
        "    retrieved = []\n",
        "    for idx in set(top_idx_embed.tolist() + top_idx_tfidf.tolist()):\n",
        "        retrieved.append(chunks[idx])\n",
        "\n",
        "    return retrieved\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "144084da",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Œ Sentence Chunking: [\"As a user, when I try to reset my password,\\nthe system should send an OTP to my registered email. Currently, OTP is not delivered if the email contains a '+' symbol.\", 'Steps:\\n1. Open login page\\n2.', 'Click forgot password\\n3. Enter email\\nExpected: OTP should be delivered\\nActual:', 'Error displayed']\n",
            "ðŸ“Œ Fixed Chunking: ['As a user, when I try to reset my password,', 'the system should send an OTP to my registered email.', 'Currently, OTP is not delivered if the email contains a', \"'+' symbol. Steps: 1. Open login page 2. Click forgot\", 'password 3. Enter email Expected: OTP should be delivered Actual:', 'Error displayed']\n",
            "ðŸ“Œ Schematic Chunking: [\"As a user, when I try to reset my password,\\nthe system should send an OTP to my registered email.\\nCurrently, OTP is not delivered if the email contains a '+' symbol.\\nSteps:\", 'Open login page', 'Click forgot password', 'Enter email\\nExpected: OTP should be delivered\\nActual: Error displayed']\n",
            "ðŸ“Œ Code Chunking: [\"As a user, when I try to reset my password,\\nthe system should send an OTP to my registered email.\\nCurrently, OTP is not delivered if the email contains a '+' symbol.\\nSteps:\\n1. Open login page\\n2. Click forgot password\\n3. Enter email\\nExpected: OTP should be delivered\\nActual: Error displayed\"]\n",
            "ðŸ“Œ Semantic Chunking: ['As a user, when I try to reset my password,\\nthe system should send an OTP to my registered email.', \"Currently, OTP is not delivered if the email contains a '+' symbol.\", 'Steps:\\n1.', 'Open login page\\n2.', 'Click forgot password\\n3.', 'Enter email\\nExpected: OTP should be delivered\\nActual:', 'Error displayed']\n"
          ]
        }
      ],
      "source": [
        "jira_text = \"\"\"\n",
        "As a user, when I try to reset my password,\n",
        "the system should send an OTP to my registered email.\n",
        "Currently, OTP is not delivered if the email contains a '+' symbol.\n",
        "Steps:\n",
        "1. Open login page\n",
        "2. Click forgot password\n",
        "3. Enter email\n",
        "Expected: OTP should be delivered\n",
        "Actual: Error displayed\n",
        "\"\"\"\n",
        "\n",
        "# Try different chunking styles\n",
        "print(\"ðŸ“Œ Sentence Chunking:\", chunking_agent(jira_text, method=\"sentence\", max_sentences=2))\n",
        "print(\"ðŸ“Œ Fixed Chunking:\", chunking_agent(jira_text, method=\"fixed\", chunk_size=10))\n",
        "print(\"ðŸ“Œ Schematic Chunking:\", chunking_agent(jira_text, method=\"schematic\"))\n",
        "print(\"ðŸ“Œ Code Chunking:\", chunking_agent(jira_text, method=\"code\"))\n",
        "print(\"ðŸ“Œ Semantic Chunking:\", chunking_agent(jira_text, method=\"semantic\", threshold=0.75))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fa910f0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Intent Understanding Prompt Template for JIRA\n",
        "def get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria):\n",
        "    return f\"\"\"\n",
        "    You are an Intent Understanding Agent for JIRA tickets. \n",
        "    Your job is to deeply analyze the given JIRA and extract the intent behind it. \n",
        "    Focus on the relationship between the JIRA heading, description, and acceptance criteria.\n",
        "\n",
        "    JIRA Input:\n",
        "    - Heading: {jira_heading}\n",
        "    - Description: {jira_description}\n",
        "    - Acceptance Criteria: {jira_acceptance_criteria}\n",
        "\n",
        "    Tasks:\n",
        "    1. Summarize the **core business intent** of this JIRA in 2â€“3 sentences.\n",
        "    2. Identify the **main functional requirements** based on the description and acceptance criteria.\n",
        "    3. Explain how the **acceptance criteria relates** to the description (what part of the description it validates).\n",
        "    4. List possible **edge cases** or missing points not explicitly covered in the acceptance criteria.\n",
        "    5. Provide a **clear conceptual understanding** of the JIRA that could be used later for test case generation.\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "91a447f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_jira_from_txt(filepath):\n",
        "    heading, description, acceptance = \"\", \"\", \"\"\n",
        "    with open(filepath, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    section = None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.lower().startswith(\"heading:\"):\n",
        "            heading = line.split(\":\", 1)[1].strip()\n",
        "            section = \"heading\"\n",
        "        elif line.lower().startswith(\"description:\"):\n",
        "            description = line.split(\":\", 1)[1].strip()\n",
        "            section = \"description\"\n",
        "        elif line.lower().startswith(\"acceptance criteria:\"):\n",
        "            acceptance = line.split(\":\", 1)[1].strip()\n",
        "            section = \"acceptance\"\n",
        "        else:\n",
        "            if section == \"description\":\n",
        "                description += \" \" + line\n",
        "            elif section == \"acceptance\":\n",
        "                acceptance += \" \" + line\n",
        "    \n",
        "    return heading, description, acceptance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    You are an Intent Understanding Agent for JIRA tickets. \n",
            "    Your job is to deeply analyze the given JIRA and extract the intent behind it. \n",
            "    Focus on the relationship between the JIRA heading, description, and acceptance criteria.\n",
            "\n",
            "    JIRA Input:\n",
            "    - Heading: Login with OTP Verification\n",
            "    - Description:  As a user, I want to log into the system using my registered mobile number. After entering the username and password, the system should send an OTP to the registered mobile. The user must enter the OTP within 2 minutes. If OTP verification fails, the login should be denied. \n",
            "    - Acceptance Criteria:  1. System sends OTP to the registered mobile number after successful username and password validation. 2. OTP must be valid for 2 minutes. 3. If OTP is incorrect or expired, user should not be logged in. 4. Successful OTP entry grants access to the system dashboard.\n",
            "\n",
            "    Tasks:\n",
            "    1. Summarize the **core business intent** of this JIRA in 2â€“3 sentences.\n",
            "    2. Identify the **main functional requirements** based on the description and acceptance criteria.\n",
            "    3. Explain how the **acceptance criteria relates** to the description (what part of the description it validates).\n",
            "    4. List possible **edge cases** or missing points not explicitly covered in the acceptance criteria.\n",
            "    5. Provide a **clear conceptual understanding** of the JIRA that could be used later for test case generation.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "jira_heading, jira_description, jira_acceptance_criteria = read_jira_from_txt(\"jira_ticket.txt\")\n",
        "intent_prompt = get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria)\n",
        "\n",
        "print(intent_prompt)  # to verify\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "42c6028f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build prompt from TXT file JIRA input\n",
        "jira_heading, jira_description, jira_acceptance_criteria = read_jira_from_txt(\"jira_ticket.txt\")\n",
        "intent_prompt = get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d12e854",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'client' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m intent_prompt = get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Send to LLM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      7\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with Ollama/local model if needed\u001b[39;00m\n\u001b[32m      8\u001b[39m     messages=[\n\u001b[32m      9\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful AI agent for JIRA analysis.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     10\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: intent_prompt}\n\u001b[32m     11\u001b[39m     ]\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“Œ JIRA Analysis:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
            "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
          ]
        }
      ],
      "source": [
        "# Build prompt from TXT file JIRA input\n",
        "jira_heading, jira_description, jira_acceptance_criteria = read_jira_from_txt(\"jira_ticket.txt\")\n",
        "intent_prompt = get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria)\n",
        "\n",
        "# Send to LLM\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",  # Replace with Ollama/local model if needed\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI agent for JIRA analysis.\"},\n",
        "        {\"role\": \"user\", \"content\": intent_prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"ðŸ“Œ JIRA Analysis:\\n\")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "358df157",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/vigneshv/Coding'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4291a4",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
