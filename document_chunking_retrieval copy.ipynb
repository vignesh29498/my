{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìù Document Chunking, Embedding, and Retrieval\n",
        "This notebook demonstrates different chunking methods (fixed, sentence, schematic, code) using **spaCy** and **regex**.\n",
        "We build TF-IDF vectors and store them locally (no external DBs).\n",
        "Then we retrieve relevant chunks based on input queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (51163182.py, line 2)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip3 install scikit-learn spacy\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Install required dependencies\n",
        "pip3 install scikit-learn spacy\n",
        "python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# ‚úÖ Load spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    raise OSError(\"spaCy model 'en_core_web_sm' not found. Run: python3 -m spacy download en_core_web_sm\")\n",
        "\n",
        "# ‚úÖ Fixed-size word chunks\n",
        "def fixed_chunk(text, chunk_size=200):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# ‚úÖ Sentence-based chunks (spaCy)\n",
        "def sentence_chunk(text, max_sentences=3):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    return [\n",
        "        \" \".join(sentences[i:i+max_sentences])\n",
        "        for i in range(0, len(sentences), max_sentences)\n",
        "    ]\n",
        "\n",
        "# ‚úÖ Schematic chunks (split by headings/numbers)\n",
        "def schematic_chunk(text):\n",
        "    parts = re.split(r'(?m)^#{1,6}\\s.*|^\\d+\\.\\s', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# ‚úÖ Code chunks (split ``` blocks)\n",
        "def code_chunk(text):\n",
        "    parts = re.split(r'```.*?```', text, flags=re.S)\n",
        "    return [p.strip() for p in parts if p.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîπ Fixed chunk: ['1. User enters username and password. System validates input. Then', 'the system sends an OTP. The user enters the OTP.', 'System verifies and logs in. ```python print(\"Hello World\") ```']\n",
            "üîπ Sentence chunk: ['1. User enters username and password.', 'System validates input. Then the system sends an OTP.', 'The user enters the OTP. System verifies and logs in.', '```python\\nprint(\"Hello World\")\\n```']\n",
            "üîπ Schematic chunk: ['User enters username and password.\\nSystem validates input.\\nThen the system sends an OTP.\\nThe user enters the OTP.\\nSystem verifies and logs in.\\n\\n```python\\nprint(\"Hello World\")\\n```']\n",
            "üîπ Code chunk: ['1. User enters username and password.\\nSystem validates input.\\nThen the system sends an OTP.\\nThe user enters the OTP.\\nSystem verifies and logs in.']\n"
          ]
        }
      ],
      "source": [
        "# üìÑ Example input text\n",
        "text = \"\"\"\n",
        "1. User enters username and password.\n",
        "System validates input.\n",
        "Then the system sends an OTP.\n",
        "The user enters the OTP.\n",
        "System verifies and logs in.\n",
        "\n",
        "```python\n",
        "print(\"Hello World\")\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîπ Fixed chunk:\", fixed_chunk(text, chunk_size=10))\n",
        "print(\"üîπ Sentence chunk:\", sentence_chunk(text, max_sentences=2))\n",
        "print(\"üîπ Schematic chunk:\", schematic_chunk(text))\n",
        "print(\"üîπ Code chunk:\", code_chunk(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö° Build TF-IDF index\n",
        "def build_tfidf_index(chunks):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "def retrieve(query, chunks, vectorizer, tfidf_matrix, top_k=3):\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
        "    top_indices = similarities.argsort()[::-1][:top_k]\n",
        "    return [(chunks[i], similarities[i]) for i in top_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: How does the system login the user?\n",
            "\n",
            "üîπ Score: 0.6588\n",
            "The user enters the OTP. System verifies and logs in.\n",
            "\n",
            "üîπ Score: 0.4358\n",
            "System validates input. Then the system sends an OTP.\n",
            "\n",
            "üîπ Score: 0.1637\n",
            "1. User enters username and password.\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Choose chunking method\n",
        "chunks = sentence_chunk(text, max_sentences=2)\n",
        "\n",
        "# ‚úÖ Build TF-IDF index\n",
        "vectorizer, tfidf_matrix = build_tfidf_index(chunks)\n",
        "\n",
        "# üîç Query\n",
        "query = \"How does the system login the user?\"\n",
        "results = retrieve(query, chunks, vectorizer, tfidf_matrix)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "for chunk, score in results:\n",
        "    print(f\"\\nüîπ Score: {score:.4f}\\n{chunk}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "47440e23",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ‚úÖ Embedding model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load sentence transformer (local)\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ‚úÖ Load spaCy English model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    raise OSError(\"spaCy model 'en_core_web_sm' not found. Run: python3 -m spacy download en_core_web_sm\")\n",
        "\n",
        "# ---------- Chunking Methods ----------\n",
        "def fixed_chunk(text, chunk_size=200):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "def sentence_chunk(text, max_sentences=3):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    return [\" \".join(sentences[i:i+max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
        "\n",
        "def schematic_chunk(text):\n",
        "    parts = re.split(r'(?m)^#{1,6}\\s.*|^\\d+\\.\\s', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "def code_chunk(text):\n",
        "    parts = re.split(r'```.*?```', text, flags=re.S)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# ---------- Build Vector Index ----------\n",
        "def build_vector_index(chunks):\n",
        "    # Embedding for each chunk\n",
        "    embeddings = embedder.encode(chunks)\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# ---------- TF-IDF Index ----------\n",
        "def build_tfidf_index(chunks):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "    return vectorizer, tfidf_matrix\n",
        "\n",
        "# ---------- Retrieval ----------\n",
        "def retrieve(query, chunks, embeddings, tfidf_vectorizer, tfidf_matrix, top_k=3):\n",
        "    # Query embedding\n",
        "    query_vec = embedder.encode([query])\n",
        "\n",
        "    # Cosine similarity with embedding vectors\n",
        "    sim_scores = cosine_similarity(query_vec, embeddings)[0]\n",
        "    top_idx_embed = np.argsort(sim_scores)[::-1][:top_k]\n",
        "\n",
        "    # TF-IDF similarity\n",
        "    tfidf_query = tfidf_vectorizer.transform([query])\n",
        "    tfidf_scores = cosine_similarity(tfidf_query, tfidf_matrix)[0]\n",
        "    top_idx_tfidf = np.argsort(tfidf_scores)[::-1][:top_k]\n",
        "\n",
        "    print(\"\\nüîπ Embedding-based results:\")\n",
        "    for idx in top_idx_embed:\n",
        "        print(f\"- {chunks[idx][:80]}... (score: {sim_scores[idx]:.4f})\")\n",
        "\n",
        "    print(\"\\nüîπ TF-IDF results:\")\n",
        "    for idx in top_idx_tfidf:\n",
        "        print(f\"- {chunks[idx][:80]}... (score: {tfidf_scores[idx]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fc456d21",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîπ Embedding-based results:\n",
            "- System validates OTP and logs in.... (score: 0.6073)\n",
            "- 1. User enters username and password.... (score: 0.5722)\n",
            "- 2. System validates input.... (score: 0.4879)\n",
            "\n",
            "üîπ TF-IDF results:\n",
            "- User enters OTP. 5.... (score: 0.4692)\n",
            "- 1. User enters username and password.... (score: 0.3122)\n",
            "- 3. System sends OTP.\n",
            "4.... (score: 0.3106)\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "1. User enters username and password.\n",
        "2. System validates input.\n",
        "3. System sends OTP.\n",
        "4. User enters OTP.\n",
        "5. System validates OTP and logs in.\n",
        "\"\"\"\n",
        "\n",
        "# Choose chunking method\n",
        "chunks = sentence_chunk(text, max_sentences=2)\n",
        "\n",
        "# Build indexes\n",
        "embeddings = build_vector_index(chunks)\n",
        "tfidf_vectorizer, tfidf_matrix = build_tfidf_index(chunks)\n",
        "\n",
        "# Query\n",
        "retrieve(\"How does the system validate user login?\", chunks, embeddings, tfidf_vectorizer, tfidf_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4683725",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91c70963",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d47ba91",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba62a70",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b5a4adc5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load spaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    raise OSError(\"Run: python3 -m spacy download en_core_web_sm\")\n",
        "\n",
        "# Embedding model\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# ---------- Different Chunking Methods ----------\n",
        "\n",
        "# 1. Fixed-size word chunks\n",
        "def fixed_chunk(text, chunk_size=200):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "\n",
        "# 2. Sentence-based chunks (spaCy)\n",
        "def sentence_chunk(text, max_sentences=3):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    return [\" \".join(sentences[i:i+max_sentences]) for i in range(0, len(sentences), max_sentences)]\n",
        "\n",
        "# 3. Schematic chunks (headings / numbered lists)\n",
        "def schematic_chunk(text):\n",
        "    parts = re.split(r'(?m)^#{1,6}\\s.*|^\\d+\\.\\s', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# 4. Code-aware chunks (split ``` blocks)\n",
        "def code_chunk(text):\n",
        "    parts = re.split(r'```.*?```', text, flags=re.S)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "# 5. Semantic chunks (embedding-based, greedy merge)\n",
        "def semantic_chunk(text, threshold=0.7):\n",
        "    \"\"\"\n",
        "    Uses embeddings to merge semantically similar sentences into chunks.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "    embeddings = embedder.encode(sentences)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = [sentences[0]]\n",
        "    prev_vec = embeddings[0]\n",
        "\n",
        "    for i in range(1, len(sentences)):\n",
        "        sim = cosine_similarity([prev_vec], [embeddings[i]])[0][0]\n",
        "        if sim >= threshold:  # semantically close ‚Üí same chunk\n",
        "            current_chunk.append(sentences[i])\n",
        "        else:  # new semantic group\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [sentences[i]]\n",
        "        prev_vec = embeddings[i]\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# ---------- Chunking Agent ----------\n",
        "def chunking_agent(text, method=\"sentence\", **kwargs):\n",
        "    if method == \"fixed\":\n",
        "        return fixed_chunk(text, **kwargs)\n",
        "    elif method == \"sentence\":\n",
        "        return sentence_chunk(text, **kwargs)\n",
        "    elif method == \"schematic\":\n",
        "        return schematic_chunk(text)\n",
        "    elif method == \"code\":\n",
        "        return code_chunk(text)\n",
        "    elif method == \"semantic\":\n",
        "        return semantic_chunk(text, **kwargs)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown chunking method: {method}\")\n",
        "\n",
        "# ---------- Indexing ----------\n",
        "def build_indexes(chunks):\n",
        "    embeddings = embedder.encode(chunks)\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(chunks)\n",
        "    return embeddings, vectorizer, tfidf_matrix\n",
        "\n",
        "# ---------- Retrieval ----------\n",
        "def retrieve(query, chunks, embeddings, vectorizer, tfidf_matrix, top_k=3):\n",
        "    query_vec = embedder.encode([query])\n",
        "    sim_scores = cosine_similarity(query_vec, embeddings)[0]\n",
        "    top_idx_embed = np.argsort(sim_scores)[::-1][:top_k]\n",
        "\n",
        "    tfidf_query = vectorizer.transform([query])\n",
        "    tfidf_scores = cosine_similarity(tfidf_query, tfidf_matrix)[0]\n",
        "    top_idx_tfidf = np.argsort(tfidf_scores)[::-1][:top_k]\n",
        "\n",
        "    retrieved = []\n",
        "    for idx in set(top_idx_embed.tolist() + top_idx_tfidf.tolist()):\n",
        "        retrieved.append(chunks[idx])\n",
        "\n",
        "    return retrieved\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "144084da",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìå Sentence Chunking: [\"As a user, when I try to reset my password,\\nthe system should send an OTP to my registered email. Currently, OTP is not delivered if the email contains a '+' symbol.\", 'Steps:\\n1. Open login page\\n2.', 'Click forgot password\\n3. Enter email\\nExpected: OTP should be delivered\\nActual:', 'Error displayed']\n",
            "üìå Fixed Chunking: ['As a user, when I try to reset my password,', 'the system should send an OTP to my registered email.', 'Currently, OTP is not delivered if the email contains a', \"'+' symbol. Steps: 1. Open login page 2. Click forgot\", 'password 3. Enter email Expected: OTP should be delivered Actual:', 'Error displayed']\n",
            "üìå Schematic Chunking: [\"As a user, when I try to reset my password,\\nthe system should send an OTP to my registered email.\\nCurrently, OTP is not delivered if the email contains a '+' symbol.\\nSteps:\", 'Open login page', 'Click forgot password', 'Enter email\\nExpected: OTP should be delivered\\nActual: Error displayed']\n",
            "üìå Code Chunking: [\"As a user, when I try to reset my password,\\nthe system should send an OTP to my registered email.\\nCurrently, OTP is not delivered if the email contains a '+' symbol.\\nSteps:\\n1. Open login page\\n2. Click forgot password\\n3. Enter email\\nExpected: OTP should be delivered\\nActual: Error displayed\"]\n",
            "üìå Semantic Chunking: ['As a user, when I try to reset my password,\\nthe system should send an OTP to my registered email.', \"Currently, OTP is not delivered if the email contains a '+' symbol.\", 'Steps:\\n1.', 'Open login page\\n2.', 'Click forgot password\\n3.', 'Enter email\\nExpected: OTP should be delivered\\nActual:', 'Error displayed']\n"
          ]
        }
      ],
      "source": [
        "jira_text = \"\"\"\n",
        "As a user, when I try to reset my password,\n",
        "the system should send an OTP to my registered email.\n",
        "Currently, OTP is not delivered if the email contains a '+' symbol.\n",
        "Steps:\n",
        "1. Open login page\n",
        "2. Click forgot password\n",
        "3. Enter email\n",
        "Expected: OTP should be delivered\n",
        "Actual: Error displayed\n",
        "\"\"\"\n",
        "\n",
        "# Try different chunking styles\n",
        "print(\"üìå Sentence Chunking:\", chunking_agent(jira_text, method=\"sentence\", max_sentences=2))\n",
        "print(\"üìå Fixed Chunking:\", chunking_agent(jira_text, method=\"fixed\", chunk_size=10))\n",
        "print(\"üìå Schematic Chunking:\", chunking_agent(jira_text, method=\"schematic\"))\n",
        "print(\"üìå Code Chunking:\", chunking_agent(jira_text, method=\"code\"))\n",
        "print(\"üìå Semantic Chunking:\", chunking_agent(jira_text, method=\"semantic\", threshold=0.75))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fa910f0f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Intent Understanding Prompt Template for JIRA\n",
        "def get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria):\n",
        "    return f\"\"\"\n",
        "    You are an Intent Understanding Agent for JIRA tickets. \n",
        "    Your job is to deeply analyze the given JIRA and extract the intent behind it. \n",
        "    Focus on the relationship between the JIRA heading, description, and acceptance criteria.\n",
        "\n",
        "    JIRA Input:\n",
        "    - Heading: {jira_heading}\n",
        "    - Description: {jira_description}\n",
        "    - Acceptance Criteria: {jira_acceptance_criteria}\n",
        "\n",
        "    Tasks:\n",
        "    1. Summarize the **core business intent** of this JIRA in 2‚Äì3 sentences.\n",
        "    2. Identify the **main functional requirements** based on the description and acceptance criteria.\n",
        "    3. Explain how the **acceptance criteria relates** to the description (what part of the description it validates).\n",
        "    4. List possible **edge cases** or missing points not explicitly covered in the acceptance criteria.\n",
        "    5. Provide a **clear conceptual understanding** of the JIRA that could be used later for test case generation.\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "91a447f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_jira_from_txt(filepath):\n",
        "    heading, description, acceptance = \"\", \"\", \"\"\n",
        "    with open(filepath, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    section = None\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.lower().startswith(\"heading:\"):\n",
        "            heading = line.split(\":\", 1)[1].strip()\n",
        "            section = \"heading\"\n",
        "        elif line.lower().startswith(\"description:\"):\n",
        "            description = line.split(\":\", 1)[1].strip()\n",
        "            section = \"description\"\n",
        "        elif line.lower().startswith(\"acceptance criteria:\"):\n",
        "            acceptance = line.split(\":\", 1)[1].strip()\n",
        "            section = \"acceptance\"\n",
        "        else:\n",
        "            if section == \"description\":\n",
        "                description += \" \" + line\n",
        "            elif section == \"acceptance\":\n",
        "                acceptance += \" \" + line\n",
        "    \n",
        "    return heading, description, acceptance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    You are an Intent Understanding Agent for JIRA tickets. \n",
            "    Your job is to deeply analyze the given JIRA and extract the intent behind it. \n",
            "    Focus on the relationship between the JIRA heading, description, and acceptance criteria.\n",
            "\n",
            "    JIRA Input:\n",
            "    - Heading: Login with OTP Verification\n",
            "    - Description:  As a user, I want to log into the system using my registered mobile number. After entering the username and password, the system should send an OTP to the registered mobile. The user must enter the OTP within 2 minutes. If OTP verification fails, the login should be denied. \n",
            "    - Acceptance Criteria:  1. System sends OTP to the registered mobile number after successful username and password validation. 2. OTP must be valid for 2 minutes. 3. If OTP is incorrect or expired, user should not be logged in. 4. Successful OTP entry grants access to the system dashboard.\n",
            "\n",
            "    Tasks:\n",
            "    1. Summarize the **core business intent** of this JIRA in 2‚Äì3 sentences.\n",
            "    2. Identify the **main functional requirements** based on the description and acceptance criteria.\n",
            "    3. Explain how the **acceptance criteria relates** to the description (what part of the description it validates).\n",
            "    4. List possible **edge cases** or missing points not explicitly covered in the acceptance criteria.\n",
            "    5. Provide a **clear conceptual understanding** of the JIRA that could be used later for test case generation.\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "jira_heading, jira_description, jira_acceptance_criteria = read_jira_from_txt(\"jira_ticket.txt\")\n",
        "intent_prompt = get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria)\n",
        "\n",
        "print(intent_prompt)  # to verify\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "42c6028f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build prompt from TXT file JIRA input\n",
        "jira_heading, jira_description, jira_acceptance_criteria = read_jira_from_txt(\"jira_ticket.txt\")\n",
        "intent_prompt = get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d12e854",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'client' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m intent_prompt = get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Send to LLM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mclient\u001b[49m.chat.completions.create(\n\u001b[32m      7\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with Ollama/local model if needed\u001b[39;00m\n\u001b[32m      8\u001b[39m     messages=[\n\u001b[32m      9\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mYou are a helpful AI agent for JIRA analysis.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m     10\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: intent_prompt}\n\u001b[32m     11\u001b[39m     ]\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìå JIRA Analysis:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
            "\u001b[31mNameError\u001b[39m: name 'client' is not defined"
          ]
        }
      ],
      "source": [
        "# Build prompt from TXT file JIRA input\n",
        "jira_heading, jira_description, jira_acceptance_criteria = read_jira_from_txt(\"jira_ticket.txt\")\n",
        "intent_prompt = get_intent_prompt(jira_heading, jira_description, jira_acceptance_criteria)\n",
        "\n",
        "# Send to LLM\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",  # Replace with Ollama/local model if needed\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI agent for JIRA analysis.\"},\n",
        "        {\"role\": \"user\", \"content\": intent_prompt}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"üìå JIRA Analysis:\\n\")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "358df157",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/vigneshv/Coding'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb4291a4",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
