{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gz2z8gYrdRUM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "import joblib\n",
        "\n",
        "def load_and_merge_data(label_file):\n",
        "    labels = pd.read_csv(label_file)\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    for _, row in labels.iterrows():\n",
        "        raw_file = os.path.join('/content/drive/MyDrive/CITI/NEW/Handoff', row['Handoff_File'])\n",
        "        meaning_file = os.path.join('/content/drive/MyDrive/CITI/NEW/ICD', row['ICD_File'])\n",
        "\n",
        "        if not os.path.exists(raw_file) or not os.path.exists(meaning_file):\n",
        "            print(f\"File {raw_file} or {meaning_file} does not exist.\")\n",
        "            continue\n",
        "\n",
        "        raw_columns = row['Handoff_Columns'].split()\n",
        "        meaning_columns = row['ICD_Columns'].split()\n",
        "\n",
        "        raw_data = pd.read_csv(raw_file)\n",
        "        meaning_data = pd.read_csv(meaning_file)\n",
        "\n",
        "        # Debug: print columns of each file\n",
        "        print(f\"Raw file '{raw_file}' columns: {raw_data.columns}\")\n",
        "        print(f\"Meaning file '{meaning_file}' columns: {meaning_data.columns}\")\n",
        "\n",
        "        # Renaming columns based on label file\n",
        "        raw_data.rename(columns=dict(zip(raw_columns, ['text_id'] + raw_columns[1:])), inplace=True)\n",
        "        meaning_data.rename(columns=dict(zip(meaning_columns, ['text_id'] + meaning_columns[1:])), inplace=True)\n",
        "\n",
        "        if 'text_id' not in raw_data.columns or 'text_id' not in meaning_data.columns:\n",
        "            print(f\"Missing 'text_id' in {raw_file} or {meaning_file}\")\n",
        "            continue\n",
        "\n",
        "        # Ensure all columns are strings before concatenation\n",
        "        raw_data['Raw Text'] = raw_data[raw_columns[1:]].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "        combined_data = pd.merge(raw_data[['text_id', 'Raw Text']], meaning_data[['text_id'] + meaning_columns[1:]], on='text_id')\n",
        "        all_data.append(combined_data)\n",
        "\n",
        "    if all_data:\n",
        "        return pd.concat(all_data, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if no data was combined\n",
        "\n",
        "def preprocess_data(data):\n",
        "    if data.empty:\n",
        "        print(\"No data to preprocess. Exiting.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Convert 'Raw Text' column to string\n",
        "    data['Raw Text'] = data['Raw Text'].astype(str)\n",
        "\n",
        "    # Combine the meanings into a single column for training\n",
        "    meaning_cols = [col for col in data.columns if col.startswith('ICD_C')]\n",
        "    data['Meaning'] = data[meaning_cols].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "    X = data['Raw Text']\n",
        "    y = data['Meaning']\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def train_model(X, y):\n",
        "    if X is None or y is None:\n",
        "        print(\"No data to train. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    # Build a pipeline with TF-IDF Vectorizer and Logistic Regression\n",
        "    model_pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer()),\n",
        "        ('clf', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Debug: Print model accuracy on the test set\n",
        "    accuracy = model_pipeline.score(X_test, y_test)\n",
        "    print(f\"Model accuracy on test set: {accuracy:.2f}\")\n",
        "\n",
        "    # Save the trained model to a file using joblib\n",
        "    model_filename = 'text_pattern_model.pkl'  # Name of the model file\n",
        "    model_path = os.path.join('/content/drive/MyDrive/CITI/NEW', model_filename)  # Replace with your desired path\n",
        "    joblib.dump(model_pipeline, model_path)\n",
        "    print(f\"Trained model saved to {model_path}\")\n",
        "\n",
        "    return model_pipeline\n",
        "\n",
        "def predict_meanings(model, new_raw_file, output_file, label_file):\n",
        "    if model is None:\n",
        "        print(\"Model not available. Exiting.\")\n",
        "        return\n",
        "\n",
        "    labels = pd.read_csv(label_file)\n",
        "\n",
        "    new_raw_data = pd.read_csv(new_raw_file)\n",
        "    raw_file_label = labels[labels['Handoff_File'] == os.path.basename(new_raw_file)]\n",
        "\n",
        "    if raw_file_label.empty:\n",
        "        print(f\"No label entry found for {new_raw_file}. Exiting.\")\n",
        "        return\n",
        "\n",
        "    raw_columns = raw_file_label.iloc[0]['Handoff_Columns'].split()\n",
        "\n",
        "    raw_id_col = [col for col in new_raw_data.columns if col == raw_columns[0]]\n",
        "    if raw_id_col:\n",
        "        new_raw_data.rename(columns={raw_id_col[0]: 'text_id'}, inplace=True)\n",
        "\n",
        "    if 'text_id' not in new_raw_data.columns:\n",
        "        print(f\"New raw file is missing 'text_id' column: {new_raw_file}\")\n",
        "        return\n",
        "\n",
        "    # Ensure all columns are strings before concatenation\n",
        "    new_raw_data['Raw Text'] = new_raw_data[raw_columns[1:]].astype(str).agg(' '.join, axis=1)\n",
        "\n",
        "    # Predict meanings using the trained model\n",
        "    predicted_meanings = model.predict(new_raw_data['Raw Text'])\n",
        "\n",
        "    # Add predicted meanings to the new_raw_data DataFrame\n",
        "    new_raw_data['Predicted Meaning'] = predicted_meanings\n",
        "\n",
        "    # Save predictions to an output file\n",
        "    new_raw_data.to_csv(output_file, index=False)\n",
        "    print(f\"Predictions saved to {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    label_file = '/content/drive/MyDrive/CITI/NEW/label_file.csv'\n",
        "    raw_folder = '/content/drive/MyDrive/CITI/NEW/Handoff'\n",
        "    meaning_folder = '/content/drive/MyDrive/CITI/NEW/ICD'\n",
        "    new_raw_file = '/content/drive/MyDrive/CITI/NEW/Handoff/handoff_2.csv'\n",
        "    output_file = '/content/drive/MyDrive/CITI/NEW/predicted_meanings.csv'\n",
        "\n",
        "    # Load and merge data\n",
        "    data = load_and_merge_data(label_file)\n",
        "\n",
        "    if data.empty:\n",
        "        print(\"No data to process. Please check your input files.\")\n",
        "    else:\n",
        "        # Preprocess data\n",
        "        X, y = preprocess_data(data)\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(X, y)\n",
        "\n",
        "        # Predict and save results from the new raw file\n",
        "        predict_meanings(model, new_raw_file, output_file, label_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hPg4YqnRibdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fNY8RGbGibZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UENa68S8ibXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTddMBu1ibUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "def train_model(label_file):\n",
        "    labels = pd.read_csv(label_file)\n",
        "\n",
        "    for idx, row in labels.iterrows():\n",
        "        handoff_file = row['Handoff_File']\n",
        "        icd_file = row['ICD_File']\n",
        "        handoff_columns = row['Handoff_Columns'].split()\n",
        "        icd_columns = row['ICD_Columns'].split()\n",
        "\n",
        "        # Load Handoff data\n",
        "        handoff_data = pd.read_csv(f'/content/drive/MyDrive/CITI/NEW/Handoff/{handoff_file}', usecols=handoff_columns)\n",
        "        handoff_data.fillna('', inplace=True)\n",
        "\n",
        "        # Load ICD data\n",
        "        icd_data = pd.read_csv(f'/content/drive/MyDrive/CITI/NEW/ICD/{icd_file}', usecols=icd_columns)\n",
        "        icd_data.fillna('', inplace=True)\n",
        "\n",
        "        # Combine the text columns\n",
        "        handoff_data['combined_text'] = handoff_data.apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
        "\n",
        "        # Create a combined DataFrame for training\n",
        "        combined_data = handoff_data[['Text_ID', 'combined_text']].merge(icd_data, on='Text_ID')\n",
        "\n",
        "        # Prepare the training data\n",
        "        X = combined_data['combined_text']\n",
        "        y = combined_data.drop(columns=['Text_ID', 'combined_text'])\n",
        "\n",
        "        for column in y.columns:\n",
        "            model = make_pipeline(TfidfVectorizer(), LogisticRegression(max_iter=1000))\n",
        "            model.fit(X, y[column])\n",
        "\n",
        "            # Save the model\n",
        "            model_filename = f'model_{handoff_file.split(\".\")[0]}_{column}.pkl'\n",
        "            joblib.dump(model, model_filename)\n",
        "            print(f'Saved model to {model_filename}')\n",
        "\n",
        "def predict_meanings(label_file, new_raw_file, output_file):\n",
        "    labels = pd.read_csv(label_file)\n",
        "    handoff_columns = None\n",
        "\n",
        "    for idx, row in labels.iterrows():\n",
        "        if row['Handoff_File'] == os.path.basename(new_raw_file):\n",
        "            handoff_columns = row['Handoff_Columns'].split()\n",
        "            icd_columns = row['ICD_Columns'].split()\n",
        "            break\n",
        "\n",
        "    if handoff_columns is None:\n",
        "        raise ValueError(\"No matching entry found in the label file for the provided new_raw_file.\")\n",
        "\n",
        "    new_data = pd.read_csv(new_raw_file, usecols=handoff_columns)\n",
        "    new_data.fillna('', inplace=True)\n",
        "    new_data['combined_text'] = new_data.apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
        "\n",
        "    predictions = pd.DataFrame(new_data['Text_ID'])\n",
        "\n",
        "    for column in icd_columns[1:]:\n",
        "        model_filename = f'model_{os.path.basename(new_raw_file).split(\".\")[0]}_{column}.pkl'\n",
        "        if not os.path.exists(model_filename):\n",
        "            raise ValueError(f\"Model file {model_filename} does not exist.\")\n",
        "\n",
        "        model = joblib.load(model_filename)\n",
        "        predictions[column] = model.predict(new_data['combined_text'])\n",
        "\n",
        "    icd_data = pd.read_csv(f'/content/drive/MyDrive/CITI/NEW/ICD/{row[\"ICD_File\"]}', usecols=icd_columns)\n",
        "    icd_data = icd_data[icd_columns]  # Keep only the relevant columns\n",
        "\n",
        "    result = predictions.merge(icd_data, on='Text_ID')\n",
        "    result.drop_duplicates(inplace=True)\n",
        "\n",
        "    result.to_csv(output_file, index=False)\n",
        "    print(f'Saved predictions to {output_file}')\n",
        "\n",
        "# Example usage\n",
        "# train_model('label_file.csv')\n",
        "# predict_meanings('label_file.csv', 'AIML/Handoff/handoff_1.csv', 'predicted_output.csv')\n"
      ],
      "metadata": {
        "id": "U9Cd08QSibSW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model('/content/drive/MyDrive/CITI/NEW/label_file.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tigZF14ifgd",
        "outputId": "bea5bfe3-268d-4ff8-8c3b-11c7c1075b21"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to model_handoff_1_ICD_C1.pkl\n",
            "Saved model to model_handoff_1_ICD_C2.pkl\n",
            "Saved model to model_handoff_2_ICD_C1.pkl\n",
            "Saved model to model_handoff_2_ICD_C2.pkl\n",
            "Saved model to model_handoff_3_ICD_C1.pkl\n",
            "Saved model to model_handoff_4_ICD_C2.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_meanings('/content/drive/MyDrive/CITI/NEW/label_file.csv', '/content/drive/MyDrive/CITI/NEW/Handoff/handoff_1.csv', 'predicted_output.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6BdQEm0ijFJ",
        "outputId": "e564ec25-3fda-4d08-a105-3376807d7c9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved predictions to predicted_output.csv\n"
          ]
        }
      ]
    }
  ]
}